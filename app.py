import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import random
import pickle
from collections import Counter
import re

# Try to import scikit-learn, fallback gracefully if not available
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    st.warning("‚ö†Ô∏è scikit-learn not installed. AI features will be disabled. Install with: pip install scikit-learn")

import warnings
warnings.filterwarnings('ignore')

# Page configuration
st.set_page_config(
    page_title="ÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖÿ≥ŸÑŸÖ - Islamic Adhkar AI",
    page_icon="üïå",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for beautiful styling
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        text-align: center;
        color: white;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
    }
    
    .ai-badge {
        background: linear-gradient(45deg, #ff6b6b, #feca57);
        color: white;
        padding: 0.3rem 0.8rem;
        border-radius: 15px;
        font-size: 0.8rem;
        font-weight: bold;
        display: inline-block;
        margin-left: 0.5rem;
    }
    
    .adhkar-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        margin-bottom: 1rem;
        border-right: 4px solid #667eea;
        direction: rtl;
        text-align: right;
    }
    
    .similar-adhkar-card {
        background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        margin-bottom: 1rem;
        border-right: 4px solid #e17055;
        direction: rtl;
        text-align: right;
    }
    
    .adhkar-text {
        font-size: 1.3rem;
        line-height: 2;
        color: #2c3e50;
        font-family: 'Amiri', serif;
        margin-bottom: 1rem;
    }
    
    .similarity-score {
        background: linear-gradient(45deg, #00b894, #00cec9);
        color: white;
        padding: 0.2rem 0.6rem;
        border-radius: 12px;
        font-size: 0.8rem;
        display: inline-block;
        margin-left: 0.5rem;
    }
    
    .category-tag {
        background: linear-gradient(45deg, #667eea, #764ba2);
        color: white;
        padding: 0.3rem 1rem;
        border-radius: 20px;
        font-size: 0.9rem;
        display: inline-block;
        margin-top: 0.5rem;
    }
    
    .search-container {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    
    .ai-search-container {
        background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    
    .sidebar-content {
        background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
    }
    
    .time-based-greeting {
        background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin-bottom: 1rem;
        color: #333;
    }
    
    .counter-display {
        background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        padding: 2rem;
        border-radius: 10px;
        text-align: center;
        margin: 1rem 0;
    }
    
    .counter-number {
        font-size: 3rem;
        font-weight: bold;
        color: #2c3e50;
        margin: 1rem 0;
    }
    
    .random-adhkar {
        background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
        padding: 2rem;
        border-radius: 10px;
        margin: 1rem 0;
        text-align: center;
        color: #333;
    }
    
    .ml-insights {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    
    .stat-box {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        text-align: center;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        min-width: 150px;
    }
    
    .installation-guide {
        background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        border-left: 4px solid #e17055;
    }
</style>

<link href="https://fonts.googleapis.com/css2?family=Amiri:wght@400;700&display=swap" rel="stylesheet">
""", unsafe_allow_html=True)

@st.cache_data
def load_data():
    """Load and cache the adhkar data"""
    try:
        df = pd.read_csv('adhkar_df.csv')
        return df.dropna()
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return pd.DataFrame()

@st.cache_resource
def load_tfidf_model():
    """Load and cache the TF-IDF vectorizer model"""
    if not SKLEARN_AVAILABLE:
        return None
    
    try:
        with open('tfidf_vectorizer.pkl', 'rb') as f:
            vectorizer = pickle.load(f)
        return vectorizer
    except FileNotFoundError:
        st.warning("‚ö†Ô∏è ŸÖŸÑŸÅ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ: tfidf_vectorizer.pkl")
        return None
    except Exception as e:
        st.error(f"ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨: {e}")
        return None

@st.cache_data
def get_tfidf_matrix(_vectorizer, texts):
    """Get TF-IDF matrix for texts"""
    if not SKLEARN_AVAILABLE or _vectorizer is None:
        return None
    
    try:
        return _vectorizer.transform(texts)
    except Exception as e:
        st.error(f"ÿÆÿ∑ÿ£ ŸÅŸä ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÜÿµŸàÿµ: {e}")
        return None

def semantic_search(query, vectorizer, tfidf_matrix, df, top_k=5):
    """Perform semantic search using TF-IDF similarity"""
    if not SKLEARN_AVAILABLE:
        return pd.DataFrame(), []
    
    try:
        if vectorizer is None or tfidf_matrix is None:
            return pd.DataFrame(), []
        
        # Transform query
        query_vector = vectorizer.transform([query])
        
        # Calculate cosine similarity
        similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
        
        # Get top k most similar adhkar
        top_indices = similarities.argsort()[-top_k:][::-1]
        top_similarities = similarities[top_indices]
        
        # Filter out very low similarities
        meaningful_indices = [idx for idx, sim in zip(top_indices, top_similarities) if sim > 0.1]
        meaningful_similarities = [sim for sim in top_similarities if sim > 0.1]
        
        if not meaningful_indices:
            return pd.DataFrame(), []
            
        result_df = df.iloc[meaningful_indices].copy()
        return result_df, meaningful_similarities
        
    except Exception as e:
        st.error(f"ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä: {e}")
        return pd.DataFrame(), []

def find_similar_adhkar(adhkar_text, vectorizer, tfidf_matrix, df, top_k=3):
    """Find similar adhkar to a given adhkar"""
    if not SKLEARN_AVAILABLE:
        return pd.DataFrame(), []
    
    try:
        if vectorizer is None or tfidf_matrix is None:
            return pd.DataFrame(), []
            
        # Find the index of current adhkar
        current_idx = df[df['clean_text'] == adhkar_text].index
        if len(current_idx) == 0:
            return pd.DataFrame(), []
        
        current_idx = current_idx[0]
        
        # Get similarity with all other adhkar
        current_vector = tfidf_matrix[current_idx]
        similarities = cosine_similarity(current_vector, tfidf_matrix).flatten()
        
        # Remove self-similarity and get top k
        similarities[current_idx] = -1
        top_indices = similarities.argsort()[-top_k:][::-1]
        top_similarities = similarities[top_indices]
        
        # Filter meaningful similarities
        meaningful_indices = [idx for idx, sim in zip(top_indices, top_similarities) if sim > 0.1]
        meaningful_similarities = [sim for sim in top_similarities if sim > 0.1]
        
        if not meaningful_indices:
            return pd.DataFrame(), []
            
        result_df = df.iloc[meaningful_indices].copy()
        return result_df, meaningful_similarities
        
    except Exception as e:
        st.error(f"ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿ£ÿ∞ŸÉÿßÿ± ŸÖÿ¥ÿßÿ®Ÿáÿ©: {e}")
        return pd.DataFrame(), []

def get_category_insights(df, vectorizer, tfidf_matrix):
    """Get ML insights about categories"""
    if not SKLEARN_AVAILABLE:
        return {}
    
    try:
        if vectorizer is None or tfidf_matrix is None:
            return {}
        
        insights = {}
        categories = df['category'].unique()
        
        # Calculate average TF-IDF scores for each category
        for category in categories:
            category_mask = df['category'] == category
            category_tfidf = tfidf_matrix[category_mask]
            if category_tfidf.shape[0] > 0:
                avg_tfidf = np.mean(category_tfidf.toarray(), axis=0)
                # Get top features for this category
                feature_names = vectorizer.get_feature_names_out()
                top_features_idx = avg_tfidf.argsort()[-5:][::-1]
                top_features = [feature_names[idx] for idx in top_features_idx]
                top_scores = [avg_tfidf[idx] for idx in top_features_idx]
                
                insights[category] = {
                    'top_features': top_features,
                    'scores': top_scores,
                    'count': sum(category_mask)
                }
        
        return insights
    except Exception as e:
        st.error(f"ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÅÿ¶ÿßÿ™: {e}")
        return {}

def get_time_based_greeting():
    """Get appropriate greeting based on current time"""
    current_hour = datetime.now().hour
    
    if 5 <= current_hour < 12:
        return "üåÖ ÿµÿ®ÿßÿ≠ ÿßŸÑÿÆŸäÿ± - ÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑÿµÿ®ÿßÿ≠", "morning"
    elif 12 <= current_hour < 18:
        return "‚òÄÔ∏è ŸÖÿ≥ÿßÿ° ÿßŸÑÿÆŸäÿ± - ÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖÿ≥ÿßÿ°", "afternoon"
    elif 18 <= current_hour < 22:
        return "üåÜ ŸÖÿ≥ÿßÿ° ÿßŸÑÿÆŸäÿ± - ÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖÿ≥ÿßÿ°", "evening"
    else:
        return "üåô ÿ™ÿµÿ®ÿ≠ ÿπŸÑŸâ ÿÆŸäÿ± - ÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÜŸàŸÖ", "night"

def initialize_session_state():
    """Initialize session state variables"""
    if 'counter' not in st.session_state:
        st.session_state.counter = 0
    if 'daily_adhkar_count' not in st.session_state:
        st.session_state.daily_adhkar_count = 0
    if 'favorite_adhkar' not in st.session_state:
        st.session_state.favorite_adhkar = []
    if 'last_date' not in st.session_state:
        st.session_state.last_date = datetime.now().date()
    
    # Reset daily counter if it's a new day
    if st.session_state.last_date != datetime.now().date():
        st.session_state.daily_adhkar_count = 0
        st.session_state.last_date = datetime.now().date()

def display_adhkar_card(adhkar_text, category, index, similarity_score=None, is_similar=False):
    """Display a single adhkar card with optional similarity score"""
    card_class = "similar-adhkar-card" if is_similar else "adhkar-card"
    
    similarity_badge = ""
    if similarity_score is not None:
        similarity_percentage = int(similarity_score * 100)
        similarity_badge = f'<span class="similarity-score">ÿ™ÿ¥ÿßÿ®Ÿá: {similarity_percentage}%</span>'
    
    with st.container():
        st.markdown(f"""
        <div class="{card_class}">
            <div class="adhkar-text">{adhkar_text}</div>
            <div class="category-tag">{category}{similarity_badge}</div>
        </div>
        """, unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
        with col1:
            if st.button("üìñ ŸÇÿ±ÿßÿ°ÿ©", key=f"read_{index}"):
                st.session_state.counter += 1
                st.session_state.daily_adhkar_count += 1
                st.success("‚úÖ ÿ™ŸÖ ÿßÿ≠ÿ™ÿ≥ÿßÿ® ÿßŸÑŸÇÿ±ÿßÿ°ÿ©")
        
        with col2:
            if st.button("‚ù§Ô∏è ŸÖŸÅÿ∂ŸÑÿ©", key=f"fav_{index}"):
                if adhkar_text not in st.session_state.favorite_adhkar:
                    st.session_state.favorite_adhkar.append(adhkar_text)
                    st.success("‚úÖ ÿ™ŸÖ ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑÿ∞ŸÉÿ± ŸÑŸÑŸÖŸÅÿ∂ŸÑÿ©")
                else:
                    st.info("Ÿáÿ∞ÿß ÿßŸÑÿ∞ŸÉÿ± ŸÖŸàÿ¨ŸàÿØ ÿ®ÿßŸÑŸÅÿπŸÑ ŸÅŸä ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©")
        
        with col3:
            if SKLEARN_AVAILABLE and st.button("üîç ŸÖÿ¥ÿßÿ®Ÿá", key=f"similar_{index}"):
                st.session_state.current_adhkar_for_similarity = adhkar_text
                st.rerun()
        
        with col4:
            if st.button("üìã ŸÜÿ≥ÿÆ", key=f"copy_{index}"):
                st.code(adhkar_text, language="text")

def show_installation_guide():
    """Show installation guide for missing dependencies"""
    st.markdown("""
    <div class="installation-guide">
        <h3>üõ†Ô∏è ÿØŸÑŸäŸÑ ÿßŸÑÿ™ÿ´ÿ®Ÿäÿ™ ŸÑŸÑŸÖŸäÿ≤ÿßÿ™ ÿßŸÑÿ∞ŸÉŸäÿ©</h3>
        <p>ŸÑÿ™ŸÅÿπŸäŸÑ ÿßŸÑŸÖŸäÿ≤ÿßÿ™ ÿßŸÑÿ∞ŸÉŸäÿ©ÿå Ÿäÿ±ÿ¨Ÿâ ÿ™ÿ´ÿ®Ÿäÿ™ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©:</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.code("""
# ÿ™ÿ´ÿ®Ÿäÿ™ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©
pip install scikit-learn

# ÿ£Ÿà ÿ™ÿ´ÿ®Ÿäÿ™ ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™
pip install streamlit pandas numpy scikit-learn
    """, language="bash")
    
    st.markdown("""
    **ÿßŸÑŸÖŸäÿ≤ÿßÿ™ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ© ÿ®ÿπÿØ ÿßŸÑÿ™ÿ´ÿ®Ÿäÿ™:**
    - ü§ñ ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿ∞ŸÉŸä ÿ®ÿßŸÑŸÖÿπŸÜŸâ
    - üîç ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿ£ÿ∞ŸÉÿßÿ± ŸÖÿ¥ÿßÿ®Ÿáÿ©
    - üìä ÿ™ÿ≠ŸÑŸäŸÑÿßÿ™ ÿ∞ŸÉŸäÿ© ŸÑŸÑŸÅÿ¶ÿßÿ™
    - üéØ ÿ™ŸàÿµŸäÿßÿ™ ŸÖÿÆÿµÿµÿ©
    """)

def main():
    # Initialize session state
    initialize_session_state()
    
    # Load data and model
    df = load_data()
    vectorizer = load_tfidf_model() if SKLEARN_AVAILABLE else None
    
    if df.empty:
        st.error("ŸÑÿß ŸäŸÖŸÉŸÜ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™. Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ŸÖŸÑŸÅ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™.")
        return
    
    # Get TF-IDF matrix
    tfidf_matrix = None
    if vectorizer is not None and SKLEARN_AVAILABLE:
        with st.spinner("ÿ¨ÿßÿ±Ÿä ÿ™ÿ≠ÿ∂Ÿäÿ± ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ∞ŸÉŸä..."):
            tfidf_matrix = get_tfidf_matrix(vectorizer, df['clean_text'].tolist())
    
    # Main header
    ai_status = "ü§ñ ŸÖŸÅÿπŸÑ" if (SKLEARN_AVAILABLE and vectorizer is not None) else "‚ùå ÿ∫Ÿäÿ± ŸÖÿ™ÿßÿ≠"
    st.markdown(f"""
    <div class="main-header">
        <h1>üïå ÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖÿ≥ŸÑŸÖ ÿßŸÑÿ∞ŸÉŸä - Islamic Adhkar AI</h1>
        <p>ÿßÿ∞ŸÉÿ±Ÿàÿß ÿßŸÑŸÑŸá ŸÉÿ´Ÿäÿ±ÿßŸã ŸÑÿπŸÑŸÉŸÖ ÿ™ŸÅŸÑÿ≠ŸàŸÜ</p>
        <span class="ai-badge">ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä: {ai_status}</span>
    </div>
    """, unsafe_allow_html=True)
    
    # Time-based greeting
    greeting, time_period = get_time_based_greeting()
    st.markdown(f"""
    <div class="time-based-greeting">
        <h3>{greeting}</h3>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.markdown("""
        <div class="sidebar-content">
            <h2>ü§ñ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ∞ŸÉŸä</h2>
        </div>
        """, unsafe_allow_html=True)
        
        if SKLEARN_AVAILABLE and vectorizer is not None:
            st.success("‚úÖ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ¨ÿßŸáÿ≤ ŸÑŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ")
            vocab_size = len(vectorizer.get_feature_names_out())
            st.info(f"üìä ÿ≠ÿ¨ŸÖ ÿßŸÑŸÖŸÅÿ±ÿØÿßÿ™: {vocab_size:,} ŸÉŸÑŸÖÿ©")
        elif SKLEARN_AVAILABLE:
            st.warning("‚ö†Ô∏è ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ∫Ÿäÿ± ŸÖÿ≠ŸÖŸÑ")
        else:
            st.error("‚ùå scikit-learn ÿ∫Ÿäÿ± ŸÖÿ´ÿ®ÿ™")
        
        st.markdown("""
        <div class="sidebar-content">
            <h2>üìä ÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™ŸÉ ÿßŸÑŸäŸàŸÖŸäÿ©</h2>
        </div>
        """, unsafe_allow_html=True)
        
        # Daily statistics
        st.markdown(f"""
        <div class="counter-display">
            <h3>üéØ ÿπÿØÿßÿØ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸäŸàŸÖ</h3>
            <div class="counter-number">{st.session_state.daily_adhkar_count}</div>
            <p>ÿ∞ŸÉÿ± ŸÖŸÇÿ±Ÿàÿ° ÿßŸÑŸäŸàŸÖ</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown(f"""
        <div class="counter-display">
            <h3>üìà ÿ•ÿ¨ŸÖÿßŸÑŸä ÿßŸÑÿ£ÿ∞ŸÉÿßÿ±</h3>
            <div class="counter-number">{st.session_state.counter}</div>
            <p>ÿ•ÿ¨ŸÖÿßŸÑŸä ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖŸÇÿ±Ÿàÿ°ÿ©</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Reset counter
        if st.button("üîÑ ÿ•ÿπÿßÿØÿ© ÿ™ÿπŸäŸäŸÜ ÿßŸÑÿπÿØÿßÿØ"):
            st.session_state.counter = 0
            st.session_state.daily_adhkar_count = 0
            st.success("ÿ™ŸÖ ÿ•ÿπÿßÿØÿ© ÿ™ÿπŸäŸäŸÜ ÿßŸÑÿπÿØÿßÿØ")
        
        # AI-powered or random adhkar
        st.markdown("""
        <div class="sidebar-content">
            <h3>üéØ ÿ∞ŸÉÿ± ŸÖŸÇÿ™ÿ±ÿ≠</h3>
        </div>
        """, unsafe_allow_html=True)
        
        if st.button("üéØ ÿßÿ≠ÿµŸÑ ÿπŸÑŸâ ÿ∞ŸÉÿ±"):
            if SKLEARN_AVAILABLE and vectorizer is not None and tfidf_matrix is not None:
                # Get time-based recommendation
                current_hour = datetime.now().hour
                if 5 <= current_hour < 12:
                    query = "ÿµÿ®ÿßÿ≠"
                elif 18 <= current_hour < 22:
                    query = "ŸÖÿ≥ÿßÿ°"
                else:
                    query = "ŸÜŸàŸÖ"
                
                smart_results, similarities = semantic_search(query, vectorizer, tfidf_matrix, df, top_k=1)
                if not smart_results.empty:
                    smart_adhkar = smart_results.iloc[0]
                    st.markdown(f"""
                    <div class="random-adhkar">
                        <div class="adhkar-text">{smart_adhkar['clean_text']}</div>
                        <div class="category-tag">{smart_adhkar['category']} 
                        <span class="similarity-score">ÿ∞ŸÉŸä ü§ñ</span></div>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    random_adhkar = df.sample(1).iloc[0]
                    st.markdown(f"""
                    <div class="random-adhkar">
                        <div class="adhkar-text">{random_adhkar['clean_text']}</div>
                        <div class="category-tag">{random_adhkar['category']}</div>
                    </div>
                    """, unsafe_allow_html=True)
            else:
                random_adhkar = df.sample(1).iloc[0]
                st.markdown(f"""
                <div class="random-adhkar">
                    <div class="adhkar-text">{random_adhkar['clean_text']}</div>
                    <div class="category-tag">{random_adhkar['category']}</div>
                </div>
                """, unsafe_allow_html=True)
    
    # Main content tabs
    if SKLEARN_AVAILABLE and vectorizer is not None:
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
            "ü§ñ ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿ∞ŸÉŸä", 
            "üîç ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿ™ŸÇŸÑŸäÿØŸä", 
            "‚≠ê ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©", 
            "üìä ÿ™ÿ≠ŸÑŸäŸÑÿßÿ™ ÿ∞ŸÉŸäÿ©", 
            "üéØ ÿ™ŸàÿµŸäÿßÿ™", 
            "‚ÑπÔ∏è ÿ≠ŸàŸÑ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ"
        ])
    else:
        tab1, tab2, tab3, tab4 = st.tabs([
            "üîç ÿßŸÑÿ®ÿ≠ÿ´", 
            "‚≠ê ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©", 
            "üìä ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™",
            "‚ÑπÔ∏è ÿ≠ŸàŸÑ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ"
        ])
    
    # AI Search Tab (only if sklearn is available)
    if SKLEARN_AVAILABLE and vectorizer is not None:
        with tab1:
            st.markdown("""
            <div class="ai-search-container">
                <h3>ü§ñ ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿ∞ŸÉŸä ÿ®ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä</h3>
                <p>ÿßÿ®ÿ≠ÿ´ ÿ®ÿßŸÑŸÖÿπŸÜŸâ ŸàŸÑŸäÿ≥ ŸÅŸÇÿ∑ ÿ®ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿØŸÇŸäŸÇÿ©</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Semantic search
            semantic_query = st.text_input(
                "üß† ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿØŸÑÿßŸÑŸä", 
                placeholder="ŸÖÿ´ÿßŸÑ: ÿßŸÑÿ≠ŸÖÿßŸäÿ© ŸÖŸÜ ÿßŸÑÿ¥ÿ±ÿå ÿßŸÑÿØÿπÿßÿ° ŸÑŸÑŸàÿßŸÑÿØŸäŸÜÿå ÿßŸÑÿßÿ≥ÿ™ÿ∫ŸÅÿßÿ±...",
                help="ÿßÿ®ÿ≠ÿ´ ÿ®ÿßŸÑŸÖÿπŸÜŸâ - ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ≥ŸäŸÅŸáŸÖ ŸÇÿµÿØŸÉ"
            )
            
            col1, col2 = st.columns([1, 1])
            with col1:
                search_depth = st.selectbox("ÿπŸÖŸÇ ÿßŸÑÿ®ÿ≠ÿ´", [3, 5, 8, 10], index=1)
            with col2:
                min_similarity = st.slider("ÿ≠ÿØ ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑÿ£ÿØŸÜŸâ", 0.1, 0.8, 0.2, 0.1)
            
            if semantic_query:
                with st.spinner("ü§ñ ÿ¨ÿßÿ±Ÿä ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿ∞ŸÉŸä..."):
                    semantic_results, similarities = semantic_search(
                        semantic_query, vectorizer, tfidf_matrix, df, top_k=search_depth
                    )
                    
                    if not semantic_results.empty:
                        # Filter by minimum similarity
                        valid_indices = [i for i, sim in enumerate(similarities) if sim >= min_similarity]
                        if valid_indices:
                            filtered_results = semantic_results.iloc[valid_indices]
                            filtered_similarities = [similarities[i] for i in valid_indices]
                            
                            st.success(f"üéØ ÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ {len(filtered_results)} ŸÜÿ™Ÿäÿ¨ÿ© ÿ∞ŸÉŸäÿ©")
                            
                            for idx, (_, row) in enumerate(filtered_results.iterrows()):
                                display_adhkar_card(
                                    row['clean_text'], 
                                    row['category'], 
                                    f"semantic_{idx}",
                                    similarity_score=filtered_similarities[idx],
                                    is_similar=True
                                )
                        else:
                            st.warning("ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ŸÜÿ™ÿßÿ¶ÿ¨ ÿ™ÿ™ÿ¨ÿßŸàÿ≤ ÿ≠ÿØ ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑŸÖÿ≠ÿØÿØ")
                    else:
                        st.info("ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ŸÜÿ™ÿßÿ¶ÿ¨. ÿ¨ÿ±ÿ® ŸÉŸÑŸÖÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ©.")
            
            # Quick semantic search buttons
            st.markdown("### üöÄ ÿ®ÿ≠ÿ´ ÿ≥ÿ±Ÿäÿπ")
            quick_searches = [
                "ÿßŸÑÿ≠ŸÖÿßŸäÿ© ŸàÿßŸÑÿ£ŸÖÿßŸÜ", "ÿßŸÑÿØÿπÿßÿ° ŸÑŸÑŸàÿßŸÑÿØŸäŸÜ", "ÿßŸÑÿßÿ≥ÿ™ÿ∫ŸÅÿßÿ± ŸàÿßŸÑÿ™Ÿàÿ®ÿ©", 
                "ÿßŸÑÿ≠ŸÖÿØ ŸàÿßŸÑÿ¥ŸÉÿ±", "ÿ∑ŸÑÿ® ÿßŸÑŸáÿØÿßŸäÿ©", "ÿØÿπÿßÿ° ÿßŸÑŸÖÿ±Ÿäÿ∂"
            ]
            
            cols = st.columns(3)
            for i, quick_search in enumerate(quick_searches):
                with cols[i % 3]:
                    if st.button(quick_search, key=f"quick_{i}"):
                        semantic_results, similarities = semantic_search(
                            quick_search, vectorizer, tfidf_matrix, df, top_k=3
                        )
                        if not semantic_results.empty:
                            st.write(f"**ŸÜÿ™ÿßÿ¶ÿ¨: {quick_search}**")
                            for idx, (_, row) in enumerate(semantic_results.iterrows()):
                                display_adhkar_card(
                                    row['clean_text'], 
                                    row['category'], 
                                    f"quick_{i}_{idx}",
                                    similarity_score=similarities[idx],
                                    is_similar=True
                                )
        
        traditional_tab = tab2
        favorites_tab = tab3
        analytics_tab = tab4
        recommendations_tab = tab5
        about_tab = tab6
    else:
        traditional_tab = tab1
        favorites_tab = tab2
        analytics_tab = tab3
        about_tab = tab4
    
    # Traditional Search Tab
    with traditional_tab:
        st.markdown("""
        <div class="search-container">
            <h3>üîç ÿßŸÑÿ®ÿ≠ÿ´ ŸÅŸä ÿßŸÑÿ£ÿ∞ŸÉÿßÿ±</h3>
        </div>
        """, unsafe_allow_html=True)
        
        # Traditional search and filter options
        col1, col2 = st.columns([2, 1])
        
        with col1:
            search_query = st.text_input("üîç ÿßÿ®ÿ≠ÿ´ ŸÅŸä ÿßŸÑÿ£ÿ∞ŸÉÿßÿ±", placeholder="ÿßŸÉÿ™ÿ® ŸÉŸÑŸÖÿ© ŸÑŸÑÿ®ÿ≠ÿ´...")
        
        with col2:
            categories = ['ÿßŸÑŸÉŸÑ'] + list(df['category'].unique())
            selected_category = st.selectbox("üìÇ ÿßÿÆÿ™ÿ± ÿßŸÑŸÅÿ¶ÿ©", categories)
        
        # Filter data based on search and category
        filtered_df = df.copy()
        
        if search_query:
            filtered_df = filtered_df[
                filtered_df['clean_text'].str.contains(search_query, na=False) |
                filtered_df['category'].str.contains(search_query, na=False)
            ]
        
        if selected_category != 'ÿßŸÑŸÉŸÑ':
            filtered_df = filtered_df[filtered_df['category'] == selected_category]
        
        # Display results
        st.markdown(f"**ÿπÿØÿØ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨: {len(filtered_df)}**")
        
        # Pagination
        items_per_page = 5
        total_pages = max(1, len(filtered_df) // items_per_page + (1 if len(filtered_df) % items_per_page > 0 else 0))
        
        if total_pages > 1:
            page = st.selectbox("üìÑ ÿßŸÑÿµŸÅÿ≠ÿ©", range(1, total_pages + 1))
            start_idx = (page - 1) * items_per_page
            end_idx = start_idx + items_per_page
            page_df = filtered_df.iloc[start_idx:end_idx]
        else:
            page_df = filtered_df
        
        # Display adhkar cards
        for idx, row in page_df.iterrows():
            display_adhkar_card(row['clean_text'], row['category'], f"trad_{idx}")
    
    # Favorites Tab
    with favorites_tab:
        st.markdown("## ‚≠ê ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©")
        
        if st.session_state.favorite_adhkar:
            st.success(f"ŸÑÿØŸäŸÉ {len(st.session_state.favorite_adhkar)} ÿ∞ŸÉÿ± ŸÅŸä ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©")
            
            # AI-powered similar favorites (only if sklearn available)
            if SKLEARN_AVAILABLE and vectorizer is not None and tfidf_matrix is not None:
                if st.button("ü§ñ ÿßŸÇÿ™ÿ±ÿßÿ≠ÿßÿ™ ÿ∞ŸÉŸäÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©"):
                    all_suggestions = []
                    for fav_adhkar in st.session_state.favorite_adhkar[:3]:  # Limit to avoid too many results
                        similar_results, similarities = find_similar_adhkar(
                            fav_adhkar, vectorizer, tfidf_matrix, df, top_k=2
                        )
                        if not similar_results.empty:
                            for idx, (_, row) in enumerate(similar_results.iterrows()):
                                if row['clean_text'] not in st.session_state.favorite_adhkar:
                                    all_suggestions.append((row, similarities[idx]))
                    
                    if all_suggestions:
                        st.markdown("### ü§ñ ÿßŸÇÿ™ÿ±ÿßÿ≠ÿßÿ™ ÿ∞ŸÉŸäÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ŸÖŸÅÿ∂ŸÑÿ™ŸÉ:")
                        for idx, (row, sim) in enumerate(all_suggestions[:5]):  # Show top 5
                            display_adhkar_card(
                                row['clean_text'], 
                                row['category'], 
                                f"fav_suggest_{idx}",
                                similarity_score=sim,
                                is_similar=True
                            )
                    else:
                        st.info("ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿßŸÇÿ™ÿ±ÿßÿ≠ÿßÿ™ ÿ∞ŸÉŸäÿ© ŸÖÿ™ÿßÿ≠ÿ© ÿ≠ÿßŸÑŸäÿßŸã")
            
            st.markdown("---")
            st.markdown("### üìö ÿ£ÿ∞ŸÉÿßÿ±ŸÉ ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©:")
            
            for i, adhkar in enumerate(st.session_state.favorite_adhkar):
                st.markdown(f"""
                <div class="adhkar-card">
                    <div class="adhkar-text">{adhkar}</div>
                </div>
                """, unsafe_allow_html=True)
                
                col1, col2 = st.columns([1, 1])
                with col1:
                    if st.button(f"üóëÔ∏è ÿ≠ÿ∞ŸÅ ŸÖŸÜ ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©", key=f"del_fav_{i}"):
                        st.session_state.favorite_adhkar.remove(adhkar)
                        st.rerun()
                
                with col2:
                    if SKLEARN_AVAILABLE and vectorizer is not None and tfidf_matrix is not None:
                        if st.button(f"üîç ÿ£ÿ∞ŸÉÿßÿ± ŸÖÿ¥ÿßÿ®Ÿáÿ©", key=f"similar_fav_{i}"):
                            similar_results, similarities = find_similar_adhkar(
                                adhkar, vectorizer, tfidf_matrix, df, top_k=3
                            )
                            if not similar_results.empty:
                                st.markdown(f"**ÿ£ÿ∞ŸÉÿßÿ± ŸÖÿ¥ÿßÿ®Ÿáÿ© ŸÑ:** {adhkar[:50]}...")
                                for idx, (_, row) in enumerate(similar_results.iterrows()):
                                    display_adhkar_card(
                                        row['clean_text'], 
                                        row['category'], 
                                        f"similar_to_fav_{i}_{idx}",
                                        similarity_score=similarities[idx],
                                        is_similar=True
                                    )
            
            if st.button("üóëÔ∏è ŸÖÿ≥ÿ≠ ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©"):
                st.session_state.favorite_adhkar = []
                st.success("ÿ™ŸÖ ŸÖÿ≥ÿ≠ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©")
                st.rerun()
        else:
            st.info("ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ£ÿ∞ŸÉÿßÿ± ŸÖŸÅÿ∂ŸÑÿ© ÿ≠ÿ™Ÿâ ÿßŸÑÿ¢ŸÜ. ÿ£ÿ∂ŸÅ ÿ®ÿπÿ∂ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ŸÖŸÜ ŸÇÿ≥ŸÖ ÿßŸÑÿ®ÿ≠ÿ´!")
    
    # Analytics Tab
    with analytics_tab:
        st.markdown("## üìä ÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™ Ÿàÿ™ÿ≠ŸÑŸäŸÑÿßÿ™")
        
        # Overall statistics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown(f"""
            <div class="stat-box">
                <h3>{len(df)}</h3>
                <p>ÿ•ÿ¨ŸÖÿßŸÑŸä ÿßŸÑÿ£ÿ∞ŸÉÿßÿ±</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div class="stat-box">
                <h3>{len(df['category'].unique())}</h3>
                <p>ÿπÿØÿØ ÿßŸÑŸÅÿ¶ÿßÿ™</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown(f"""
            <div class="stat-box">
                <h3>{st.session_state.daily_adhkar_count}</h3>
                <p>ÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸäŸàŸÖ</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            st.markdown(f"""
            <div class="stat-box">
                <h3>{len(st.session_state.favorite_adhkar)}</h3>
                <p>ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Category distribution
        st.markdown("### üìà ÿ™Ÿàÿ≤Ÿäÿπ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ÿ≠ÿ≥ÿ® ÿßŸÑŸÅÿ¶ÿßÿ™")
        category_counts = df['category'].value_counts()
        st.bar_chart(category_counts.head(10))
        
        # Most common categories
        st.markdown("### üèÜ ÿ£ŸÉÿ´ÿ± ÿßŸÑŸÅÿ¶ÿßÿ™ ÿ¥ŸäŸàÿπÿßŸã")
        for i, (category, count) in enumerate(category_counts.head(5).items(), 1):
            st.write(f"{i}. **{category}**: {count} ÿ∞ŸÉÿ±")
        
        # AI Insights (only if sklearn available)
        if SKLEARN_AVAILABLE and vectorizer is not None and tfidf_matrix is not None:
            st.markdown("### ü§ñ ÿ™ÿ≠ŸÑŸäŸÑÿßÿ™ ÿ∞ŸÉŸäÿ© ŸÑŸÑŸÅÿ¶ÿßÿ™")
            
            with st.spinner("ÿ¨ÿßÿ±Ÿä ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÅÿ¶ÿßÿ™..."):
                insights = get_category_insights(df, vectorizer, tfidf_matrix)
            
            if insights:
                selected_category_for_analysis = st.selectbox(
                    "ÿßÿÆÿ™ÿ± ŸÅÿ¶ÿ© ŸÑŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ∞ŸÉŸä:", 
                    list(insights.keys())
                )
                
                if selected_category_for_analysis in insights:
                    insight = insights[selected_category_for_analysis]
                    st.markdown(f"""
                    <div class="ml-insights">
                        <h4>üìä ÿ™ÿ≠ŸÑŸäŸÑ ŸÅÿ¶ÿ©: {selected_category_for_analysis}</h4>
                        <p><strong>ÿπÿØÿØ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ±:</strong> {insight['count']}</p>
                        <p><strong>ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©:</strong></p>
                        <ul>
                    """, unsafe_allow_html=True)
                    
                    for feature, score in zip(insight['top_features'], insight['scores']):
                        st.markdown(f"<li>{feature} (ÿ£ŸáŸÖŸäÿ©: {score:.3f})</li>", unsafe_allow_html=True)
                    
                    st.markdown("</ul></div>", unsafe_allow_html=True)
        
        # Text length analysis
        st.markdown("### üìè ÿ™ÿ≠ŸÑŸäŸÑ ÿ£ÿ∑ŸàÿßŸÑ ÿßŸÑŸÜÿµŸàÿµ")
        text_lengths = df['clean_text'].str.len()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ŸÖÿ™Ÿàÿ≥ÿ∑ ÿßŸÑÿ∑ŸàŸÑ", f"{text_lengths.mean():.0f} ÿ≠ÿ±ŸÅ")
        with col2:
            st.metric("ÿ£ŸÇÿµÿ± ŸÜÿµ", f"{text_lengths.min()} ÿ≠ÿ±ŸÅ")
        with col3:
            st.metric("ÿ£ÿ∑ŸàŸÑ ŸÜÿµ", f"{text_lengths.max()} ÿ≠ÿ±ŸÅ")
        
        # Text length histogram
        st.markdown("### üìä ÿ™Ÿàÿ≤Ÿäÿπ ÿ£ÿ∑ŸàÿßŸÑ ÿßŸÑŸÜÿµŸàÿµ")
        hist_data = np.histogram(text_lengths, bins=20)
        st.bar_chart(pd.DataFrame({
            'count': hist_data[0]
        }))
    
    # Recommendations Tab (only if sklearn available)
    if SKLEARN_AVAILABLE and vectorizer is not None:
        with recommendations_tab:
            st.markdown("## üéØ ÿ™ŸàÿµŸäÿßÿ™ ÿ∞ŸÉŸäÿ©")
            
            # Time-based recommendations
            st.markdown("### ‚è∞ ÿ™ŸàÿµŸäÿßÿ™ ÿ≠ÿ≥ÿ® ÿßŸÑŸàŸÇÿ™")
            current_hour = datetime.now().hour
            
            if 5 <= current_hour < 12:
                recommended_query = "ÿµÿ®ÿßÿ≠"
                st.info("üåÖ ÿßŸÑŸàŸÇÿ™ ÿßŸÑÿ¢ŸÜ ŸÖŸÜÿßÿ≥ÿ® ŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑÿµÿ®ÿßÿ≠")
            elif 18 <= current_hour < 22:
                recommended_query = "ŸÖÿ≥ÿßÿ°"
                st.info("üåÜ ÿßŸÑŸàŸÇÿ™ ÿßŸÑÿ¢ŸÜ ŸÖŸÜÿßÿ≥ÿ® ŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖÿ≥ÿßÿ°")
            elif 22 <= current_hour or current_hour < 5:
                recommended_query = "ŸÜŸàŸÖ"
                st.info("üåô ÿßŸÑŸàŸÇÿ™ ÿßŸÑÿ¢ŸÜ ŸÖŸÜÿßÿ≥ÿ® ŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÜŸàŸÖ")
            else:
                recommended_query = "ÿØÿπÿßÿ°"
                st.info("üìø ŸäŸÖŸÉŸÜŸÉ ŸÇÿ±ÿßÿ°ÿ© ÿ£Ÿä ÿ£ÿ∞ŸÉÿßÿ± ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑŸàŸÇÿ™")
            
            time_recommendations, time_similarities = semantic_search(
                recommended_query, vectorizer, tfidf_matrix, df, top_k=3
            )
            
            if not time_recommendations.empty:
                st.markdown("#### ü§ñ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖŸÇÿ™ÿ±ÿ≠ÿ© ŸÑŸáÿ∞ÿß ÿßŸÑŸàŸÇÿ™:")
                for idx, (_, row) in enumerate(time_recommendations.iterrows()):
                    display_adhkar_card(
                        row['clean_text'], 
                        row['category'], 
                        f"time_rec_{idx}",
                        similarity_score=time_similarities[idx],
                        is_similar=True
                    )
            
            # Mood-based recommendations
            st.markdown("### üé≠ ÿ™ŸàÿµŸäÿßÿ™ ÿ≠ÿ≥ÿ® ÿßŸÑŸÖÿ≤ÿßÿ¨")
            mood_options = {
                "üòä ÿ≥ÿπŸäÿØ ŸàŸÖŸÖÿ™ŸÜ": "ÿ≠ŸÖÿØ ÿ¥ŸÉÿ±",
                "üòî ÿ≠ÿ≤ŸäŸÜ ŸàŸÖŸáŸÖŸàŸÖ": "ÿ≠ÿ≤ŸÜ ŸáŸÖ",
                "üò∞ ŸÇŸÑŸÇ ŸàÿÆÿßÿ¶ŸÅ": "ÿÆŸàŸÅ ÿ£ŸÖÿßŸÜ",
                "ü§≤ ÿ±ÿßÿ∫ÿ® ŸÅŸä ÿßŸÑÿØÿπÿßÿ°": "ÿØÿπÿßÿ° ÿßÿ≥ÿ™ÿ∫ŸÅÿßÿ±",
                "üôè ÿ∑ÿßŸÑÿ® ÿßŸÑŸÖÿ∫ŸÅÿ±ÿ©": "ÿ™Ÿàÿ®ÿ© ÿßÿ≥ÿ™ÿ∫ŸÅÿßÿ±",
                "‚ù§Ô∏è ŸÖÿ≠ÿ® ŸÑŸÑŸá": "ÿ≠ÿ® ÿßŸÑŸÑŸá"
            }
            
            selected_mood = st.selectbox("ÿßÿÆÿ™ÿ± ÿ≠ÿßŸÑÿ™ŸÉ ÿßŸÑÿ≠ÿßŸÑŸäÿ©:", list(mood_options.keys()))
            
            if st.button("üéØ ÿßÿ≠ÿµŸÑ ÿπŸÑŸâ ÿ™ŸàÿµŸäÿßÿ™ ŸÑŸÑŸÖÿ≤ÿßÿ¨"):
                mood_query = mood_options[selected_mood]
                mood_recommendations, mood_similarities = semantic_search(
                    mood_query, vectorizer, tfidf_matrix, df, top_k=4
                )
                
                if not mood_recommendations.empty:
                    st.markdown(f"#### ÿ™ŸàÿµŸäÿßÿ™ ŸÑÿ≠ÿßŸÑÿ©: {selected_mood}")
                    for idx, (_, row) in enumerate(mood_recommendations.iterrows()):
                        display_adhkar_card(
                            row['clean_text'], 
                            row['category'], 
                            f"mood_rec_{idx}",
                            similarity_score=mood_similarities[idx],
                            is_similar=True
                        )
            
            # Personalized recommendations based on favorites
            if st.session_state.favorite_adhkar:
                st.markdown("### üíù ÿ™ŸàÿµŸäÿßÿ™ ÿ¥ÿÆÿµŸäÿ© ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ŸÖŸÅÿ∂ŸÑÿ™ŸÉ")
                
                if st.button("ü§ñ ÿßÿ≠ÿµŸÑ ÿπŸÑŸâ ÿ™ŸàÿµŸäÿßÿ™ ÿ¥ÿÆÿµŸäÿ©"):
                    personal_recommendations = []
                    
                    # Analyze favorite adhkar to get personalized recommendations
                    for fav_adhkar in st.session_state.favorite_adhkar[:2]:
                        similar_results, similarities = find_similar_adhkar(
                            fav_adhkar, vectorizer, tfidf_matrix, df, top_k=2
                        )
                        
                        for idx, (_, row) in enumerate(similar_results.iterrows()):
                            if row['clean_text'] not in st.session_state.favorite_adhkar:
                                personal_recommendations.append((row, similarities[idx]))
                    
                    if personal_recommendations:
                        # Sort by similarity and show top recommendations
                        personal_recommendations.sort(key=lambda x: x[1], reverse=True)
                        
                        st.markdown("#### üéØ ÿ™ŸàÿµŸäÿßÿ™ ŸÖÿÆÿµÿµÿ© ŸÑŸÉ:")
                        for idx, (row, sim) in enumerate(personal_recommendations[:4]):
                            display_adhkar_card(
                                row['clean_text'], 
                                row['category'], 
                                f"personal_rec_{idx}",
                                similarity_score=sim,
                                is_similar=True
                            )
                    else:
                        st.info("ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿ™ŸàÿµŸäÿßÿ™ ÿ¥ÿÆÿµŸäÿ© ŸÖÿ™ÿßÿ≠ÿ© ÿ≠ÿßŸÑŸäÿßŸã")
    
    # About Tab
    with about_tab:
        st.markdown("## ‚ÑπÔ∏è ÿ≠ŸàŸÑ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ")
        
        if not SKLEARN_AVAILABLE:
            show_installation_guide()
            st.markdown("---")
        
        st.markdown(f"""
        ### üïå ÿ™ÿ∑ÿ®ŸäŸÇ ÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖÿ≥ŸÑŸÖ ÿßŸÑÿ∞ŸÉŸä
        
        Ÿáÿ∞ÿß ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÖÿ¨ŸÖŸàÿπÿ© ÿ¥ÿßŸÖŸÑÿ© ŸÖŸÜ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ŸàÿßŸÑÿ£ÿØÿπŸäÿ© ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸäÿ© ÿßŸÑŸÖÿ£ÿÆŸàÿ∞ÿ© ŸÖŸÜ ÿßŸÑŸÇÿ±ÿ¢ŸÜ ÿßŸÑŸÉÿ±ŸäŸÖ ŸàÿßŸÑÿ≥ŸÜÿ© ÿßŸÑŸÜÿ®ŸàŸäÿ© ÿßŸÑÿ¥ÿ±ŸäŸÅÿ©.
        
        #### üåü ÿßŸÑŸÖŸÖŸäÿ≤ÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©:
        - üìñ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ {len(df)} ÿ∞ŸÉÿ± ŸàÿØÿπÿßÿ°
        - üîç ÿ®ÿ≠ÿ´ ÿ™ŸÇŸÑŸäÿØŸä ŸÅŸä ÿßŸÑÿ£ÿ∞ŸÉÿßÿ±
        - ‚≠ê ÿ•ŸÖŸÉÿßŸÜŸäÿ© ÿ≠ŸÅÿ∏ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©
        - üìä ÿ™ÿ™ÿ®ÿπ ÿπÿØÿØ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ÿßŸÑŸÖŸÇÿ±Ÿàÿ°ÿ©
        - üéØ ÿßŸÇÿ™ÿ±ÿßÿ≠ÿßÿ™ ÿ≠ÿ≥ÿ® ÿßŸÑŸàŸÇÿ™
        - üì± ÿ™ÿµŸÖŸäŸÖ ŸÖÿ™ÿ¨ÿßŸàÿ®
        """)
        
        if SKLEARN_AVAILABLE and vectorizer is not None:
            vocab_size = len(vectorizer.get_feature_names_out())
            st.markdown(f"""
            #### ü§ñ ÿßŸÑŸÖŸÖŸäÿ≤ÿßÿ™ ÿßŸÑÿ∞ŸÉŸäÿ© (ŸÖŸÅÿπŸÑÿ©):
            - üß† ÿ®ÿ≠ÿ´ ÿ∞ŸÉŸä ÿ®ÿßŸÑŸÖÿπŸÜŸâ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ TF-IDF
            - üîç ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿ£ÿ∞ŸÉÿßÿ± ŸÖÿ¥ÿßÿ®Ÿáÿ©
            - üìä ÿ™ÿ≠ŸÑŸäŸÑÿßÿ™ ÿ∞ŸÉŸäÿ© ŸÑŸÑŸÅÿ¶ÿßÿ™
            - üéØ ÿ™ŸàÿµŸäÿßÿ™ ŸÖÿÆÿµÿµÿ©
            - üìà ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÜÿµŸàÿµ ÿ®ŸÄ {vocab_size:,} ŸÉŸÑŸÖÿ©
            """)
        elif SKLEARN_AVAILABLE:
            st.markdown("""
            #### ‚ö†Ô∏è ÿßŸÑŸÖŸÖŸäÿ≤ÿßÿ™ ÿßŸÑÿ∞ŸÉŸäÿ© (ÿ∫Ÿäÿ± ŸÖŸÅÿπŸÑÿ©):
            - ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ∫Ÿäÿ± ŸÖÿ≠ŸÖŸÑ - ÿ™ÿ£ŸÉÿØ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ŸÖŸÑŸÅ `tfidf_vectorizer.pkl`
            """)
        else:
            st.markdown("""
            #### ‚ùå ÿßŸÑŸÖŸÖŸäÿ≤ÿßÿ™ ÿßŸÑÿ∞ŸÉŸäÿ© (ÿ∫Ÿäÿ± ŸÖÿ™ÿßÿ≠ÿ©):
            - Ÿäÿ™ÿ∑ŸÑÿ® ÿ™ÿ´ÿ®Ÿäÿ™ scikit-learn
            - ÿ±ÿßÿ¨ÿπ ÿØŸÑŸäŸÑ ÿßŸÑÿ™ÿ´ÿ®Ÿäÿ™ ÿ£ÿπŸÑÿßŸá
            """)
        
        st.markdown(f"""
        #### üìö ÿßŸÑŸÅÿ¶ÿßÿ™ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ© ({len(df['category'].unique())} ŸÅÿ¶ÿ©):
        """)
        
        categories_list = df['category'].unique()
        for i, category in enumerate(categories_list, 1):
            count = len(df[df['category'] == category])
            st.write(f"{i}. **{category}** ({count} ÿ∞ŸÉÿ±)")
        
        st.markdown("""
        ---
        
        #### üõ†Ô∏è ÿßŸÑŸÖÿ™ÿ∑ŸÑÿ®ÿßÿ™ ÿßŸÑÿ™ŸÇŸÜŸäÿ©:
        ```
        streamlit>=1.28.0
        pandas>=1.5.0
        numpy>=1.24.0
        scikit-learn>=1.3.0  # ŸÑŸÑŸÖŸäÿ≤ÿßÿ™ ÿßŸÑÿ∞ŸÉŸäÿ©
        ```
        
        #### üìÅ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©:
        - `adhkar_df.csv` - ÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ±
        - `tfidf_vectorizer.pkl` - ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ∞ŸÉŸä (ÿßÿÆÿ™Ÿäÿßÿ±Ÿä)
        
        ---
        ### ü§≤ ÿØÿπÿßÿ°
        
        *"ÿßŸÑŸÑŸáŸÖ ÿßÿ¨ÿπŸÑ Ÿáÿ∞ÿß ÿßŸÑÿπŸÖŸÑ ÿÆÿßŸÑÿµÿßŸã ŸÑŸàÿ¨ŸáŸÉ ÿßŸÑŸÉÿ±ŸäŸÖÿå ŸàÿßŸÜŸÅÿπ ÿ®Ÿá ÿßŸÑŸÖÿ≥ŸÑŸÖŸäŸÜ ŸÅŸä ŸÉŸÑ ŸÖŸÉÿßŸÜ"*
        
        **ÿ™ÿ∞ŸÉÿ±:** ÿßŸÑŸÖÿØÿßŸàŸÖÿ© ÿπŸÑŸâ ÿßŸÑÿ£ÿ∞ŸÉÿßÿ± ÿÆŸäÿ± ŸÖŸÜ ÿßŸÑÿßŸÜŸÇÿ∑ÿßÿπ ÿπŸÜŸáÿß
        
        ---
        
        #### üìû ÿßŸÑÿØÿπŸÖ ÿßŸÑÿ™ŸÇŸÜŸä:
        - ÿ•ÿ∞ÿß Ÿàÿßÿ¨Ÿáÿ™ ŸÖÿ¥ÿßŸÉŸÑ ŸÅŸä ÿßŸÑÿ™ÿ´ÿ®Ÿäÿ™ÿå ÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ•ÿµÿØÿßÿ±ÿßÿ™ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™
        - ŸÑŸÑŸÖÿ≥ÿßÿπÿØÿ© ŸÅŸä ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ∞ŸÉŸäÿå ÿ™ÿ£ŸÉÿØ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ŸÖŸÑŸÅ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨
        - ÿßŸÑŸÜÿ∑ÿ®ŸäŸÇ ŸäÿπŸÖŸÑ ÿ®ÿØŸàŸÜ ÿßŸÑŸÖŸäÿ≤ÿßÿ™ ÿßŸÑÿ∞ŸÉŸäÿ© ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ŸÉŸÜ ŸÖÿ™ÿßÿ≠ÿ©
        """)

if __name__ == "__main__":
    main()
