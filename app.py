import streamlit as st

# Page configuration MUST be first
st.set_page_config(
    page_title="ØªÙØ³ÙŠØ± Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„Ø°ÙƒÙŠ - Smart Quran Tafseer",
    page_icon="ğŸ“–",
    layout="wide",
    initial_sidebar_state="collapsed"
)

import pandas as pd
import numpy as np
from datetime import datetime
import re
from collections import Counter

# Try to import optional dependencies
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    st.warning("âš ï¸ Ù…ÙƒØªØ¨Ø© scikit-learn ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. Ù„Ù„ØªØ«Ø¨ÙŠØª: pip install scikit-learn")

try:
    from datasets import load_dataset
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False
    st.error("âŒ Ù…ÙƒØªØ¨Ø© datasets ØºÙŠØ± Ù…Ø«Ø¨ØªØ©. Ù„Ù„ØªØ«Ø¨ÙŠØª: pip install datasets")

import warnings
warnings.filterwarnings('ignore')

# Enhanced CSS
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Amiri:wght@400;700&family=Inter:wght@300;400;500;600;700&display=swap');
    
    :root {
        --primary-600: #16a34a;
        --primary-700: #15803d;
        --secondary-600: #2563eb;
        --secondary-700: #1d4ed8;
        --accent-500: #f59e0b;
        --gray-50: #f9fafb;
        --gray-100: #f3f4f6;
        --gray-500: #6b7280;
        --gray-600: #4b5563;
        --gray-700: #374151;
        --gray-800: #1f2937;
    }
    
    .stApp {
        background: linear-gradient(135deg, #f0fdf4 0%, #eff6ff 50%, #fffbeb 100%);
        font-family: 'Inter', sans-serif;
    }
    
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {visibility: hidden;}
    
    .premium-header {
        background: linear-gradient(135deg, rgba(22, 163, 74, 0.95) 0%, rgba(37, 99, 235, 0.9) 50%, rgba(217, 119, 6, 0.95) 100%);
        color: white;
        padding: 3rem 0;
        margin: -1rem -1rem 3rem -1rem;
        text-align: center;
        border-radius: 0 0 20px 20px;
    }
    
    .header-title {
        font-size: 3rem;
        font-weight: 700;
        margin-bottom: 1rem;
        font-family: 'Amiri', serif;
    }
    
    .premium-card {
        background: rgba(255, 255, 255, 0.95);
        border-radius: 20px;
        padding: 2rem;
        margin-bottom: 2rem;
        box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1);
        border: 1px solid rgba(255, 255, 255, 0.5);
    }
    
    .premium-arabic {
        font-family: 'Amiri', serif;
        font-size: 1.8rem;
        line-height: 2;
        color: var(--gray-800);
        text-align: right;
        direction: rtl;
        background: linear-gradient(135deg, rgba(22, 163, 74, 0.05) 0%, rgba(255, 255, 255, 0.8) 100%);
        padding: 2rem;
        border-radius: 15px;
        border-right: 6px solid var(--primary-600);
        margin: 1rem 0;
    }
    
    .tafseer-text {
        font-family: 'Amiri', serif;
        font-size: 1.2rem;
        line-height: 1.8;
        color: var(--gray-700);
        text-align: right;
        direction: rtl;
        background: var(--gray-50);
        padding: 1.5rem;
        border-radius: 10px;
        border-right: 4px solid var(--accent-500);
        margin: 1rem 0;
    }
    
    .ayah-number {
        background: linear-gradient(135deg, var(--primary-600), var(--primary-700));
        color: white;
        padding: 6px 12px;
        border-radius: 50%;
        font-weight: bold;
        font-size: 0.9rem;
        display: inline-block;
        margin: 0 8px;
        min-width: 30px;
        text-align: center;
    }
    
    .surah-badge {
        background: linear-gradient(135deg, var(--secondary-600), var(--secondary-700));
        color: white;
        padding: 8px 16px;
        border-radius: 20px;
        font-weight: 600;
        font-size: 0.9rem;
        display: inline-block;
        margin: 8px 4px;
    }
    
    .model-status {
        display: flex;
        gap: 1rem;
        justify-content: center;
        flex-wrap: wrap;
        margin: 1rem 0;
    }
    
    .model-badge {
        display: flex;
        align-items: center;
        gap: 6px;
        padding: 8px 16px;
        border-radius: 20px;
        font-size: 0.85rem;
        font-weight: 600;
    }
    
    .model-active {
        background: linear-gradient(135deg, var(--primary-600), var(--primary-700));
        color: white;
    }
    
    .model-inactive {
        background: var(--gray-100);
        color: var(--gray-600);
    }
</style>
""", unsafe_allow_html=True)

def remove_tashkeel(text):
    """Remove Arabic diacritics"""
    if not isinstance(text, str):
        return ""
    tashkeel_pattern = re.compile(r'[\u064B-\u065F\u0670]')
    return tashkeel_pattern.sub('', text)

def preprocess_arabic_text(text):
    """Arabic text preprocessing"""
    if not isinstance(text, str):
        return ""
    
    text = remove_tashkeel(text)
    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
    text = re.sub(r'Ø©', 'Ù‡', text)
    text = re.sub(r'ÙŠ', 'Ù‰', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def create_sample_data():
    """Create sample data as fallback"""
    sample_data = {
        'text': [
            'Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù Ø§Ù„Ø±ÙÙ‘Ø­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±ÙÙ‘Ø­ÙÙŠÙ…Ù',
            'Ø§Ù„Ù’Ø­ÙÙ…Ù’Ø¯Ù Ù„ÙÙ„ÙÙ‘Ù‡Ù Ø±ÙØ¨ÙÙ‘ Ø§Ù„Ù’Ø¹ÙØ§Ù„ÙÙ…ÙÙŠÙ†Ù',
            'Ø§Ù„Ø±ÙÙ‘Ø­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±ÙÙ‘Ø­ÙÙŠÙ…Ù',
            'Ù…ÙØ§Ù„ÙÙƒÙ ÙŠÙÙˆÙ’Ù…Ù Ø§Ù„Ø¯ÙÙ‘ÙŠÙ†Ù',
            'Ø¥ÙÙŠÙÙ‘Ø§ÙƒÙ Ù†ÙØ¹Ù’Ø¨ÙØ¯Ù ÙˆÙØ¥ÙÙŠÙÙ‘Ø§ÙƒÙ Ù†ÙØ³Ù’ØªÙØ¹ÙÙŠÙ†Ù',
            'Ø§Ù‡Ù’Ø¯ÙÙ†ÙØ§ Ø§Ù„ØµÙÙ‘Ø±ÙØ§Ø·Ù Ø§Ù„Ù’Ù…ÙØ³Ù’ØªÙÙ‚ÙÙŠÙ…Ù',
            'ØµÙØ±ÙØ§Ø·Ù Ø§Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù Ø£ÙÙ†Ù’Ø¹ÙÙ…Ù’ØªÙ Ø¹ÙÙ„ÙÙŠÙ’Ù‡ÙÙ…Ù’ ØºÙÙŠÙ’Ø±Ù Ø§Ù„Ù’Ù…ÙØºÙ’Ø¶ÙÙˆØ¨Ù Ø¹ÙÙ„ÙÙŠÙ’Ù‡ÙÙ…Ù’ ÙˆÙÙ„ÙØ§ Ø§Ù„Ø¶ÙÙ‘Ø§Ù„ÙÙ‘ÙŠÙ†Ù'
        ],
        'surah': ['Ø§Ù„ÙØ§ØªØ­Ø©'] * 7,
        'ayah_number': list(range(1, 8)),
        'tafseer': [
            'Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…: Ø§ÙØªØªØ§Ø­ ÙƒÙ„ Ø³ÙˆØ±Ø© Ø¨Ø­Ù…Ø¯ Ø§Ù„Ù„Ù‡ ÙˆØ§Ù„Ø«Ù†Ø§Ø¡ Ø¹Ù„ÙŠÙ‡',
            'Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†: Ø§Ù„Ø«Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ù‡ Ø¨ØµÙØ§ØªÙ‡ Ø§Ù„Ø¬Ù…ÙŠÙ„Ø©',
            'Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…: Ù…Ù† Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø­Ø³Ù†Ù‰',
            'Ù…Ø§Ù„Ùƒ ÙŠÙˆÙ… Ø§Ù„Ø¯ÙŠÙ†: Ø§Ù„Ù„Ù‡ Ù‡Ùˆ Ø§Ù„Ù…Ø§Ù„Ùƒ Ø§Ù„Ù…ØªØµØ±Ù ÙŠÙˆÙ… Ø§Ù„Ù‚ÙŠØ§Ù…Ø©',
            'Ø¥ÙŠØ§Ùƒ Ù†Ø¹Ø¨Ø¯ ÙˆØ¥ÙŠØ§Ùƒ Ù†Ø³ØªØ¹ÙŠÙ†: Ø§Ù„ØªÙˆØ­ÙŠØ¯ ÙÙŠ Ø§Ù„Ø¹Ø¨Ø§Ø¯Ø© ÙˆØ§Ù„Ø§Ø³ØªØ¹Ø§Ù†Ø©',
            'Ø§Ù‡Ø¯Ù†Ø§ Ø§Ù„ØµØ±Ø§Ø· Ø§Ù„Ù…Ø³ØªÙ‚ÙŠÙ…: Ø¯Ø¹Ø§Ø¡ Ø¨Ø§Ù„Ù‡Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø·Ø±ÙŠÙ‚ Ø§Ù„Ù…Ø³ØªÙ‚ÙŠÙ…',
            'ØµØ±Ø§Ø· Ø§Ù„Ø°ÙŠÙ† Ø£Ù†Ø¹Ù…Øª Ø¹Ù„ÙŠÙ‡Ù…: Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø£Ù†Ø¨ÙŠØ§Ø¡ ÙˆØ§Ù„ØµØ§Ù„Ø­ÙŠÙ†'
        ]
    }
    
    df = pd.DataFrame(sample_data)
    df['clean_text'] = df['text'].apply(preprocess_arabic_text)
    df['clean_tafseer'] = df['tafseer'].apply(preprocess_arabic_text)
    
    return df

@st.cache_data
def load_quran_dataset():
    """Load Quran dataset using the exact code you specified"""
    
    if not DATASETS_AVAILABLE:
        st.error("âŒ Ù…ÙƒØªØ¨Ø© datasets ØºÙŠØ± Ù…Ø«Ø¨ØªØ©")
        st.code("pip install datasets", language="bash")
        st.warning("ğŸ”„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©...")
        return create_sample_data()
    
    try:
        with st.spinner("ğŸ“– Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø±Ø¢Ù† Ù…Ù† Hugging Face..."):
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ Ø·Ù„Ø¨ØªÙ‡ Ø¨Ø§Ù„Ø¶Ø¨Ø·
            from datasets import load_dataset
            dataset = load_dataset("MohamedRashad/Quran-Tafseer")
            df = dataset['train'].to_pandas()
            
            st.success(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df):,} Ø³Ø¬Ù„ Ù…Ù† Hugging Face")
            
            # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ÙÙ‡Ù… Ø§Ù„Ù‡ÙŠÙƒÙ„
            st.info("ğŸ” ÙØ­Øµ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
            st.write("**Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:**", list(df.columns))
            
            # Ø¹Ø±Ø¶ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if len(df) > 0:
                st.write("**Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:**")
                st.dataframe(df.head(3))
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
            df = process_dataframe(df)
            
            return df
            
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        st.warning("ğŸ”„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©...")
        return create_sample_data()

def process_dataframe(df):
    """Process the loaded dataframe to standardize columns"""
    
    # Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
    column_mapping = {
        'ayah': 'text',
        'verse': 'text',
        'arabic': 'text',
        'quran_text': 'text',
        'verse_text': 'text',
        'surah_name': 'surah',
        'chapter': 'surah',
        'chapter_name': 'surah',
        'verse_number': 'ayah_number',
        'ayah_num': 'ayah_number',
        'verse_id': 'ayah_number',
        'tafsir': 'tafseer',
        'interpretation': 'tafseer',
        'explanation': 'tafseer'
    }
    
    # ØªØ·Ø¨ÙŠÙ‚ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
    for old_col, new_col in column_mapping.items():
        if old_col in df.columns and new_col not in df.columns:
            df[new_col] = df[old_col]
            st.info(f"ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù…ÙˆØ¯ '{old_col}' Ø¥Ù„Ù‰ '{new_col}'")
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    if 'text' not in df.columns:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø¹Ù…ÙˆØ¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ Ø¹Ø±Ø¨ÙŠ
        for col in df.columns:
            if len(df) > 0:
                sample_text = str(df[col].iloc[0])
                # ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø£Ø­Ø±Ù Ø¹Ø±Ø¨ÙŠØ©
                if any(char in sample_text for char in 'Ø§Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ'):
                    df['text'] = df[col]
                    st.success(f"ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ '{col}' ÙƒÙ†Øµ Ø§Ù„Ø¢ÙŠØ§Øª")
                    break
        
        if 'text' not in df.columns:
            st.error("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ Ø§Ù„Ø¢ÙŠØ§Øª")
            return create_sample_data()
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
    if 'surah' not in df.columns:
        df['surah'] = 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'
        st.warning("âš ï¸ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø³ÙˆØ±Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ - ØªÙ… Ø¥Ø¶Ø§ÙØ© Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©")
    
    if 'ayah_number' not in df.columns:
        df['ayah_number'] = range(1, len(df) + 1)
        st.warning("âš ï¸ Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù… Ø§Ù„Ø¢ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ - ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªØ±Ù‚ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ")
    
    if 'tafseer' not in df.columns:
        df['tafseer'] = ''
        st.warning("âš ï¸ Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªÙØ³ÙŠØ± ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ - Ø³ÙŠØªÙ… Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ ÙÙ‚Ø·")
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df['text'] = df['text'].fillna('').astype(str)
    df['tafseer'] = df['tafseer'].fillna('').astype(str)
    df['surah'] = df['surah'].fillna('ØºÙŠØ± Ù…Ø­Ø¯Ø¯').astype(str)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    df['clean_text'] = df['text'].apply(preprocess_arabic_text)
    df['clean_tafseer'] = df['tafseer'].apply(preprocess_arabic_text)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ÙØ§Ø±ØºØ©
    initial_count = len(df)
    df = df[df['clean_text'].str.strip() != ''].copy()
    df = df.reset_index(drop=True)
    final_count = len(df)
    
    if initial_count != final_count:
        st.info(f"ØªÙ… Ø¥Ø²Ø§Ù„Ø© {initial_count - final_count} ØµÙ ÙØ§Ø±Øº")
    
    # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    st.success(f"""
    âœ… **ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­:**
    - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¢ÙŠØ§Øª: {len(df):,}
    - Ø§Ù„Ø³ÙˆØ± Ø§Ù„Ù…Ø®ØªÙ„ÙØ©: {len(df['surah'].unique())}
    - Ø¢ÙŠØ§Øª Ø¨ØªÙØ³ÙŠØ±: {len(df[df['tafseer'].str.strip() != ''])}
    - Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¢ÙŠØ©: {df['text'].str.len().mean():.0f} Ø­Ø±Ù
    """)
    
    return df

def simple_text_search(query, df, top_k=10):
    """Simple text search"""
    try:
        clean_query = preprocess_arabic_text(query).lower()
        if not clean_query.strip():
            return pd.DataFrame(), []
        
        scores = []
        query_words = set(clean_query.split())
        
        for _, row in df.iterrows():
            text = preprocess_arabic_text(str(row['text'])).lower()
            tafseer = preprocess_arabic_text(str(row.get('tafseer', ''))).lower()
            combined_text = text + ' ' + tafseer
            text_words = set(combined_text.split())
            
            if query_words:
                overlap = len(query_words.intersection(text_words))
                score = overlap / len(query_words)
            else:
                score = 0
            
            scores.append(score)
        
        scores = np.array(scores)
        top_indices = scores.argsort()[-top_k:][::-1]
        top_scores = scores[top_indices]
        
        valid_mask = top_scores > 0
        if not np.any(valid_mask):
            return pd.DataFrame(), []
        
        top_indices = top_indices[valid_mask]
        top_scores = top_scores[valid_mask]
        
        result_df = df.iloc[top_indices].copy()
        return result_df, top_scores.tolist()
        
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
        return pd.DataFrame(), []

def tfidf_search(query, df, top_k=10):
    """TF-IDF search"""
    try:
        if not SKLEARN_AVAILABLE:
            return simple_text_search(query, df, top_k)
        
        clean_query = preprocess_arabic_text(query)
        if not clean_query.strip():
            return pd.DataFrame(), []
        
        texts = df['clean_text'].fillna('').tolist()
        if not texts:
            return pd.DataFrame(), []
        
        vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            min_df=1,
            max_df=0.95
        )
        
        corpus = [clean_query] + texts
        tfidf_matrix = vectorizer.fit_transform(corpus)
        query_vector = tfidf_matrix[0]
        document_vectors = tfidf_matrix[1:]
        
        similarities = cosine_similarity(query_vector, document_vectors).flatten()
        
        top_indices = similarities.argsort()[-top_k:][::-1]
        top_similarities = similarities[top_indices]
        
        valid_mask = top_similarities > 0.01
        if not np.any(valid_mask):
            return pd.DataFrame(), []
        
        top_indices = top_indices[valid_mask]
        top_similarities = top_similarities[valid_mask]
        
        result_df = df.iloc[top_indices].copy()
        return result_df, top_similarities.tolist()
        
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ: {e}")
        return simple_text_search(query, df, top_k)

def display_ayah_card(row, similarity_score=None, model_type=None, card_id=None):
    """Display ayah card"""
    if card_id is None:
        card_id = f"ayah_{hash(str(row.get('text', ''))[:50])}"
    
    with st.container():
        st.markdown('<div class="premium-card">', unsafe_allow_html=True)
        
        # Header
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            surah_name = row.get('surah', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            ayah_number = row.get('ayah_number', '')
            
            st.markdown(f"""
            <div class="surah-badge">ğŸ“– Ø³ÙˆØ±Ø© {surah_name}</div>
            <span class="ayah-number">{ayah_number}</span>
            """, unsafe_allow_html=True)
        
        with col2:
            if similarity_score is not None:
                percentage = int(similarity_score * 100)
                st.markdown(f"ğŸ¯ {percentage}%")
        
        with col3:
            if model_type:
                st.markdown(f"ğŸ” {model_type}")
        
        # Arabic text
        ayah_text = row.get('text', '')
        if ayah_text:
            st.markdown(f'<div class="premium-arabic">{ayah_text}</div>', unsafe_allow_html=True)
        
        # Tafseer
        tafseer_text = row.get('tafseer', '')
        if tafseer_text and tafseer_text.strip():
            st.markdown("**ğŸ“š Ø§Ù„ØªÙØ³ÙŠØ±:**")
            st.markdown(f'<div class="tafseer-text">{tafseer_text}</div>', unsafe_allow_html=True)
        
        # Action buttons
        st.markdown("---")
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ“š Ø­ÙØ¸", key=f"save_{card_id}", use_container_width=True):
                if 'saved_ayahs' not in st.session_state:
                    st.session_state.saved_ayahs = []
                st.session_state.saved_ayahs.append(ayah_text)
                st.success("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¢ÙŠØ©!")
        
        with col2:
            if st.button("ğŸ“‹ Ù†Ø³Ø®", key=f"copy_{card_id}", use_container_width=True):
                st.code(ayah_text, language="text")
                st.success("ğŸ“‹ ÙŠÙ…ÙƒÙ†Ùƒ Ù†Ø³Ø® Ø§Ù„Ù†Øµ Ø£Ø¹Ù„Ø§Ù‡!")
        
        st.markdown('</div>', unsafe_allow_html=True)

def initialize_session_state():
    """Initialize session state"""
    defaults = {
        'search_mode': 'tfidf',
        'saved_ayahs': [],
        'search_history': [],
        'daily_verses_read': 0,
        'total_verses_read': 0,
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

def show_model_status():
    """Show model status"""
    st.markdown('<div class="model-status">', unsafe_allow_html=True)
    
    # Dataset status
    datasets_class = "model-active" if DATASETS_AVAILABLE else "model-inactive"
    datasets_icon = "ğŸ¤—" if DATASETS_AVAILABLE else "âŒ"
    st.markdown(f'<div class="model-badge {datasets_class}"><span>{datasets_icon}</span><span>Hugging Face</span></div>', unsafe_allow_html=True)
    
    # TF-IDF status
    tfidf_class = "model-active" if SKLEARN_AVAILABLE else "model-inactive"
    tfidf_icon = "ğŸ“Š" if SKLEARN_AVAILABLE else "âŒ"
    st.markdown(f'<div class="model-badge {tfidf_class}"><span>{tfidf_icon}</span><span>TF-IDF</span></div>', unsafe_allow_html=True)
    
    # Simple search (always available)
    st.markdown('<div class="model-badge model-active"><span>ğŸ”</span><span>Ø¨Ø­Ø« Ù†ØµÙŠ</span></div>', unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)

def show_search_tab(df):
    """Search tab"""
    st.markdown("### ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† ÙˆØ§Ù„ØªÙØ³ÙŠØ±")
    
    # Search mode selection
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ“Š TF-IDF (Ø°ÙƒÙŠ)", 
                    disabled=not SKLEARN_AVAILABLE,
                    type="primary" if st.session_state.search_mode == 'tfidf' else "secondary",
                    use_container_width=True):
            st.session_state.search_mode = 'tfidf'
    
    with col2:
        if st.button("ğŸ” Ø¨Ø­Ø« Ù†ØµÙŠ", 
                    type="primary" if st.session_state.search_mode == 'simple' else "secondary",
                    use_container_width=True):
            st.session_state.search_mode = 'simple'
    
    # Search input
    col_search, col_button = st.columns([4, 1])
    
    with col_search:
        search_query = st.text_input(
            "",
            placeholder="Ø§Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† ÙˆØ§Ù„ØªÙØ³ÙŠØ±... Ù…Ø«Ø§Ù„: 'Ø§Ù„ØµØ¨Ø±'ØŒ 'Ø§Ù„Ø±Ø­Ù…Ø©'ØŒ 'Ø§Ù„Ø¬Ù†Ø©'",
            key="main_search",
            label_visibility="collapsed"
        )
    
    with col_button:
        search_pressed = st.button("ğŸ” Ø¨Ø­Ø«", use_container_width=True)
    
    # Search options
    with st.expander("âš™ï¸ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¨Ø­Ø«", expanded=False):
        max_results = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬", 5, 50, 10)
        search_in_tafseer = st.checkbox("Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªÙØ³ÙŠØ± Ø£ÙŠØ¶Ø§Ù‹", value=True)
    
    # Perform search
    if search_query and (search_pressed or search_query):
        if search_query not in st.session_state.search_history:
            st.session_state.search_history.insert(0, search_query)
            st.session_state.search_history = st.session_state.search_history[:20]
        
        with st.spinner("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø«..."):
            search_df = df.copy()
            if search_in_tafseer:
                search_df['combined_text'] = (
                    search_df['clean_text'].fillna('') + ' ' + 
                    search_df.get('clean_tafseer', '').fillna('')
                )
                search_df['clean_text'] = search_df['combined_text']
            
            if st.session_state.search_mode == 'tfidf' and SKLEARN_AVAILABLE:
                results_df, similarities = tfidf_search(search_query, search_df, max_results)
                model_type = 'TF-IDF'
            else:
                results_df, similarities = simple_text_search(search_query, search_df, max_results)
                model_type = 'Ø¨Ø­Ø« Ù†ØµÙŠ'
            
            if not results_df.empty:
                st.success(f"ğŸ¯ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results_df)} Ù†ØªÙŠØ¬Ø©")
                
                if similarities:
                    avg_similarity = np.mean(similarities) * 100
                    max_similarity = np.max(similarities) * 100
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ù‚Ø©", f"{avg_similarity:.1f}%")
                    with col2:
                        st.metric("Ø£Ø¹Ù„Ù‰ Ø¯Ù‚Ø©", f"{max_similarity:.1f}%")
                    with col3:
                        unique_surahs = len(results_df['surah'].unique())
                        st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø³ÙˆØ±", unique_surahs)
                
                for idx, (_, row) in enumerate(results_df.iterrows()):
                    similarity = similarities[idx] if idx < len(similarities) else None
                    display_ayah_card(row, similarity, model_type, f"result_{idx}")
                    
            else:
                st.markdown("""
                <div style="text-align: center; padding: 3rem; background: #f9fafb; border-radius: 20px; margin: 2rem 0;">
                    <div style="font-size: 3rem; margin-bottom: 1rem;">ğŸ”</div>
                    <h3>Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬</h3>
                    <p>Ø¬Ø±Ø¨ ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ ØºÙŠØ± Ù†ÙˆØ¹ Ø§Ù„Ø¨Ø­Ø«</p>
                </div>
                """, unsafe_allow_html=True)
    
    # Quick suggestions
    if not search_query:
        st.markdown("### ğŸš€ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ø¨Ø­Ø«")
        
        suggestions = ["Ø§Ù„ØµØ¨Ø±", "Ø§Ù„Ø±Ø­Ù…Ø©", "Ø§Ù„Ø¬Ù†Ø©", "Ø§Ù„ØªÙˆØ¨Ø©", "Ø§Ù„Ø¯Ø¹Ø§Ø¡", "Ø§Ù„Ø¹Ø¯Ù„"]
        
        cols = st.columns(3)
        for i, suggestion in enumerate(suggestions):
            with cols[i % 3]:
                if st.button(f"ğŸ” {suggestion}", key=f"suggestion_{i}", use_container_width=True):
                    st.session_state.main_search = suggestion
                    st.rerun()

def show_saved_tab():
    """Saved ayahs tab"""
    st.markdown("### ğŸ“š Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©")
    
    if not st.session_state.saved_ayahs:
        st.markdown("""
        <div style="text-align: center; padding: 3rem; background: #f9fafb; border-radius: 20px; margin: 2rem 0;">
            <div style="font-size: 3rem; margin-bottom: 1rem;">ğŸ“š</div>
            <h3>Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¢ÙŠØ§Øª Ù…Ø­ÙÙˆØ¸Ø©</h3>
            <p>Ø§Ø­ÙØ¸ Ø¢ÙŠØ§ØªÙƒ Ø§Ù„Ù…ÙØ¶Ù„Ø© Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«</p>
        </div>
        """, unsafe_allow_html=True)
        return
    
    st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©", len(st.session_state.saved_ayahs))
    
    for i, ayah in enumerate(st.session_state.saved_ayahs):
        st.markdown(f"""
        <div class="premium-card">
            <div class="premium-arabic">{ayah}</div>
        </div>
        """, unsafe_allow_html=True)

def show_stats_tab(df):
    """Statistics tab"""
    st.markdown("### ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¢ÙŠØ§Øª", f"{len(df):,}")
    
    with col2:
        unique_surahs = len(df['surah'].unique())
        st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø³ÙˆØ±", unique_surahs)
    
    with col3:
        has_tafseer = len(df[df['tafseer'].notna() & (df['tafseer'] != '')])
        st.metric("Ø¢ÙŠØ§Øª Ø¨ØªÙØ³ÙŠØ±", f"{has_tafseer:,}")
    
    with col4:
        avg_length = df['text'].str.len().mean()
        st.metric("Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¢ÙŠØ©", f"{avg_length:.0f} Ø­Ø±Ù")
    
    # User statistics
    st.markdown("#### ğŸ‘¤ Ø¥Ø­ØµØ§Ø¦ÙŠØ§ØªÙƒ Ø§Ù„Ø´Ø®ØµÙŠØ©")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©", len(st.session_state.saved_ayahs))
    
    with col2:
        st.metric("Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø«", len(st.session_state.search_history))
    
    with col3:
        st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø§Øª", st.session_state.total_verses_read)
    
    # Show some dataset insights
    if len(df) > 0:
        st.markdown("#### ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        # Most common words in dataset
        if 'clean_text' in df.columns:
            all_words = []
            for text in df['clean_text'].head(1000):  # Sample first 1000 for performance
                all_words.extend(text.split())
            
            if all_words:
                word_counts = Counter(all_words)
                most_common = word_counts.most_common(10)
                
                st.markdown("**Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹:**")
                for word, count in most_common:
                    if word and len(word) > 1:  # Skip single characters
                        st.write(f"- **{word}**: {count:,} Ù…Ø±Ø©")

def show_about_tab():
    """About tab"""
    st.markdown("### â„¹ï¸ Ø­ÙˆÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
    
    st.markdown("""
    **ğŸ¤– ØªØ·Ø¨ÙŠÙ‚ Ù…ØªØ·ÙˆØ± Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…**
    
    ÙŠØ³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªØ·ÙˆØ±Ø© Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ… ÙˆØªÙØ³ÙŠØ±Ù‡:
    
    #### ğŸ” **Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«:**
    - **TF-IDF**: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚
    - **Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ**: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ø¨Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
    - **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©**: ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    
    #### ğŸ“š **Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:**
    - **Hugging Face Dataset**: MohamedRashad/Quran-Tafseer
    - **Ø§Ù„ØªØ­Ù…ÙŠÙ„**: ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Hugging Face
    - **Ø§Ù„Ø­Ø¬Ù…**: Ø¢Ù„Ø§Ù Ø§Ù„Ø¢ÙŠØ§Øª Ù…Ø¹ Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„
    
    #### ğŸ› ï¸ **Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:**
    ```python
    from datasets import load_dataset
    dataset = load_dataset("MohamedRashad/Quran-Tafseer")
    df = dataset['train'].to_pandas()
    ```
    
    #### ğŸ“¦ **Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:**
    ```bash
    pip install streamlit pandas numpy datasets scikit-learn
    ```
    
    #### ğŸ’¡ **Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
    - Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ø­Ø¯Ø¯Ø©
    - Ø¬Ø±Ø¨ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ØªÙØ³ÙŠØ± Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø£ÙˆØ³Ø¹
    - Ø§Ø­ÙØ¸ Ø¢ÙŠØ§ØªÙƒ Ø§Ù„Ù…ÙØ¶Ù„Ø© Ù„Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„ÙŠÙ‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹
    - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
    """)
    
    # Show technical details
    st.markdown("#### ğŸ”§ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:**
        - Streamlit (ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)
        - Pandas (Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
        - Scikit-learn (Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ)
        - Datasets (ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
        """)
    
    with col2:
        st.markdown("""
        **Ø§Ù„Ù…ÙŠØ²Ø§Øª:**
        - Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ ÙˆØ¯Ù‚ÙŠÙ‚
        - ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        - Ø­ÙØ¸ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…ÙØ¶Ù„Ø©
        - Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©
        """)

def main():
    """Main application"""
    initialize_session_state()
    
    # Show loading message
    with st.spinner("ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
        df = load_quran_dataset()
    
    if df.empty:
        st.error("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†")
        st.stop()
    
    # Header
    st.markdown(f"""
    <div class="premium-header">
        <h1 class="header-title">ØªÙØ³ÙŠØ± Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„Ø°ÙƒÙŠ</h1>
        <p style="font-size: 1.2rem; margin-bottom: 1rem;">Smart Quran Tafseer with AI Search</p>
        <p>Ø¨Ø­Ø« Ø°ÙƒÙŠ ÙÙŠ {len(df):,} Ø¢ÙŠØ© Ù‚Ø±Ø¢Ù†ÙŠØ© Ù…Ø¹ Ø§Ù„ØªÙØ³ÙŠØ± Ù…Ù† Hugging Face</p>
        <p style="font-size: 0.9rem; opacity: 0.8;">Ø§Ù„Ù…ØµØ¯Ø±: MohamedRashad/Quran-Tafseer</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Show model status
    show_model_status()
    
    # Navigation
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” Ø§Ù„Ø¨Ø­Ø«", "ğŸ“š Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©", "ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª", "â„¹ï¸ Ø­ÙˆÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"])
    
    with tab1:
        show_search_tab(df)
    
    with tab2:
        show_saved_tab()
    
    with tab3:
        show_stats_tab(df)
    
    with tab4:
        show_about_tab()
    
    # Footer
    st.markdown("---")
    st.markdown(f"""
    <div style="text-align: center; color: var(--gray-500); padding: 2rem;">
        <p>ğŸ“– <strong>ØªÙØ³ÙŠØ± Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„Ø°ÙƒÙŠ</strong> - ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨Ù€ â¤ï¸ Ù„Ø®Ø¯Ù…Ø© ÙƒØªØ§Ø¨ Ø§Ù„Ù„Ù‡</p>
        <p style="font-size: 0.9rem;">Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: {datetime.now().strftime('%Y-%m-%d')}</p>
        <p style="font-size: 0.8rem;">
            Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†: 
            <a href="https://huggingface.co/datasets/MohamedRashad/Quran-Tafseer" target="_blank" style="color: var(--primary-600);">
                MohamedRashad/Quran-Tafseer
            </a> 
            Ø¹Ù„Ù‰ Hugging Face
        </p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
