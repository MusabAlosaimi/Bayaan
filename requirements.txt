# requirements.txt for Streamlit Cloud
streamlit==1.28.0
pandas==1.5.3
numpy==1.24.0
scikit-learn==1.3.0
datasets==2.14.0
huggingface-hub==0.16.0

# Optional but recommended for better performance
joblib==1.3.0

# Note: Removed heavy dependencies that might cause deployment issues:
# - transformers (very large, comment out if not using BERT)
# - sentence-transformers (very large)
# - gensim (can be large)
# - torch (extremely large)

# For local development, you can uncomment these:
# transformers>=4.30.0
# sentence-transformers>=2.2.0
# gensim>=4.3.0

---

# .streamlit/config.toml
[server]
maxUploadSize = 200
enableCORS = false
enableXsrfProtection = false

[browser]
gatherUsageStats = false

[theme]
primaryColor = "#22c55e"
backgroundColor = "#ffffff"
secondaryBackgroundColor = "#f0fdf4"
textColor = "#1f2937"
font = "sans serif"

---

# packages.txt (system packages for Streamlit Cloud)
# Add any system-level dependencies here if needed
# For example:
# libgl1-mesa-glx
# libglib2.0-0

---

# Lightweight version of app.py for Streamlit Cloud deployment
# This version removes heavy ML dependencies and focuses on core functionality

import streamlit as st

# Page configuration MUST be first
st.set_page_config(
    page_title="تفسير القرآن الذكي - Smart Quran Tafseer",
    page_icon="📖",
    layout="wide",
    initial_sidebar_state="collapsed"
)

import pandas as pd
import numpy as np
from datetime import datetime
import re
from collections import Counter

# Try to import optional dependencies with better error handling
PACKAGES_STATUS = {
    'datasets': False,
    'sklearn': False,
    'joblib': False
}

try:
    from datasets import load_dataset
    PACKAGES_STATUS['datasets'] = True
except ImportError:
    st.warning("⚠️ datasets package not available. Using fallback data.")

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    PACKAGES_STATUS['sklearn'] = True
except ImportError:
    st.warning("⚠️ scikit-learn not available. Limited search functionality.")

try:
    import joblib
    PACKAGES_STATUS['joblib'] = True
except ImportError:
    import pickle  # fallback

import warnings
warnings.filterwarnings('ignore')

# Lightweight CSS for Streamlit Cloud
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Amiri:wght@400;700&family=Inter:wght@300;400;500;600;700&display=swap');
    
    :root {
        --primary-color: #22c55e;
        --secondary-color: #3b82f6;
        --accent-color: #f59e0b;
    }
    
    .main-header {
        background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
        color: white;
        padding: 2rem;
        border-radius: 10px;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .arabic-text {
        font-family: 'Amiri', serif;
        font-size: 1.5rem;
        line-height: 2;
        text-align: right;
        direction: rtl;
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-right: 4px solid var(--primary-color);
        margin: 1rem 0;
    }
    
    .verse-card {
        border: 1px solid #e0e0e0;
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
        background: white;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    .status-badge {
        display: inline-block;
        padding: 4px 8px;
        border-radius: 12px;
        font-size: 0.8rem;
        font-weight: bold;
        margin: 2px;
    }
    
    .status-active {
        background: #dcfce7;
        color: #166534;
    }
    
    .status-inactive {
        background: #fee2e2;
        color: #991b1b;
    }
</style>
""", unsafe_allow_html=True)

def preprocess_arabic_text(text):
    """Lightweight Arabic text preprocessing"""
    if not isinstance(text, str):
        return ""
    
    # Remove diacritics
    tashkeel_pattern = re.compile(r'[\u064B-\u065F\u0670]')
    text = tashkeel_pattern.sub('', text)
    
    # Basic normalization
    text = re.sub(r'[إأآا]', 'ا', text)
    text = re.sub(r'ة', 'ه', text)
    text = re.sub(r'ي', 'ى', text)
    
    # Clean whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

@st.cache_data
def load_sample_data():
    """Create sample Quranic data for demonstration"""
    sample_verses = [
        {
            'text': 'بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ',
            'surah': 'الفاتحة',
            'ayah_number': 1,
            'tafseer': 'بسم الله الرحمن الرحيم: استعانة بالله في بداية كل عمل'
        },
        {
            'text': 'الْحَمْدُ لِلَّهِ رَبِّ الْعَالَمِينَ',
            'surah': 'الفاتحة', 
            'ayah_number': 2,
            'tafseer': 'الحمد لله: الثناء على الله بجميع محامده'
        },
        {
            'text': 'الرَّحْمَٰنِ الرَّحِيمِ',
            'surah': 'الفاتحة',
            'ayah_number': 3,
            'tafseer': 'الرحمن الرحيم: صفتان من صفات الله تدلان على سعة رحمته'
        },
        {
            'text': 'مَالِكِ يَوْمِ الدِّينِ',
            'surah': 'الفاتحة',
            'ayah_number': 4,
            'tafseer': 'مالك يوم الدين: الله المالك المتصرف يوم الجزاء'
        },
        {
            'text': 'إِيَّاكَ نَعْبُدُ وَإِيَّاكَ نَسْتَعِينُ',
            'surah': 'الفاتحة',
            'ayah_number': 5,
            'tafseer': 'إياك نعبد وإياك نستعين: توحيد الله في العبادة والاستعانة'
        },
        {
            'text': 'قُلْ هُوَ اللَّهُ أَحَدٌ',
            'surah': 'الإخلاص',
            'ayah_number': 1,
            'tafseer': 'قل هو الله أحد: إعلان توحيد الله وأنه واحد لا شريك له'
        },
        {
            'text': 'اللَّهُ الصَّمَدُ',
            'surah': 'الإخلاص',
            'ayah_number': 2,
            'tafseer': 'الله الصمد: الله الذي يصمد إليه في الحوائج'
        },
        {
            'text': 'وَاللَّيْلِ إِذَا يَغْشَىٰ',
            'surah': 'الليل',
            'ayah_number': 1,
            'tafseer': 'والليل إذا يغشى: قسم بالليل عندما يغطي الأرض'
        },
        {
            'text': 'وَالنَّهَارِ إِذَا تَجَلَّىٰ',
            'surah': 'الليل',
            'ayah_number': 2,
            'tafseer': 'والنهار إذا تجلى: قسم بالنهار عندما يظهر ويضيء'
        },
        {
            'text': 'رَبَّنَا آتِنَا فِي الدُّنْيَا حَسَنَةً وَفِي الْآخِرَةِ حَسَنَةً وَقِنَا عَذَابَ النَّارِ',
            'surah': 'البقرة',
            'ayah_number': 201,
            'tafseer': 'دعاء جامع للخير في الدنيا والآخرة والاستعاذة من النار'
        }
    ]
    
    df = pd.DataFrame(sample_verses)
    df['clean_text'] = df['text'].apply(preprocess_arabic_text)
    return df

@st.cache_data
def load_quran_data():
    """Load Quran data with fallback to sample data"""
    if not PACKAGES_STATUS['datasets']:
        st.info("📱 استخدام البيانات التجريبية - ثبت مكتبة datasets للحصول على البيانات الكاملة")
        return load_sample_data()
    
    try:
        with st.spinner("📖 جاري تحميل بيانات القرآن..."):
            # Try to load from Hugging Face
            dataset = load_dataset("MohamedRashad/Quran-Tafseer", split="train")
            df = pd.DataFrame(dataset)
            
            # Handle different column names
            if 'ayah' in df.columns and 'text' not in df.columns:
                df['text'] = df['ayah']
            elif 'verse' in df.columns and 'text' not in df.columns:
                df['text'] = df['verse']
            
            # Ensure required columns
            required_columns = ['text', 'surah', 'ayah_number']
            for col in required_columns:
                if col not in df.columns:
                    if col == 'surah':
                        df['surah'] = 'غير محدد'
                    elif col == 'ayah_number':
                        df['ayah_number'] = range(1, len(df) + 1)
            
            # Add tafseer if not present
            if 'tafseer' not in df.columns:
                df['tafseer'] = ''
            
            df['clean_text'] = df['text'].apply(preprocess_arabic_text)
            
            # Limit dataset size for demo (remove for production)
            if len(df) > 1000:
                df = df.head(1000)  # Limit to first 1000 verses for demo
            
            st.success(f"✅ تم تحميل {len(df)} آية")
            return df
            
    except Exception as e:
        st.warning(f"⚠️ خطأ في تحميل البيانات: {e}")
        st.info("🔄 استخدام البيانات التجريبية...")
        return load_sample_data()

def simple_search(query, df, max_results=10):
    """Simple text search without ML dependencies"""
    if not query or df.empty:
        return pd.DataFrame(), []
    
    query_clean = preprocess_arabic_text(query).lower()
    query_words = set(query_clean.split())
    
    if not query_words:
        return pd.DataFrame(), []
    
    scores = []
    for _, row in df.iterrows():
        text_clean = preprocess_arabic_text(str(row['text'])).lower()
        text_words = set(text_clean.split())
        
        # Calculate Jaccard similarity
        intersection = len(query_words & text_words)
        union = len(query_words | text_words)
        
        if union > 0:
            score = intersection / union
        else:
            score = 0
        
        scores.append(score)
    
    # Get top results
    df_with_scores = df.copy()
    df_with_scores['score'] = scores
    
    # Filter and sort
    results = df_with_scores[df_with_scores['score'] > 0].nlargest(max_results, 'score')
    
    return results.drop('score', axis=1), results['score'].tolist()

def tfidf_search(query, df, max_results=10):
    """TF-IDF search if sklearn is available"""
    if not PACKAGES_STATUS['sklearn']:
        return simple_search(query, df, max_results)
    
    try:
        texts = [preprocess_arabic_text(query)] + df['clean_text'].tolist()
        
        vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            min_df=1,
            max_df=0.95
        )
        
        tfidf_matrix = vectorizer.fit_transform(texts)
        query_vector = tfidf_matrix[0]
        doc_vectors = tfidf_matrix[1:]
        
        similarities = cosine_similarity(query_vector, doc_vectors).flatten()
        
        # Get top results
        top_indices = similarities.argsort()[-max_results:][::-1]
        top_scores = similarities[top_indices]
        
        # Filter by minimum similarity
        valid_mask = top_scores > 0.01
        if not valid_mask.any():
            return pd.DataFrame(), []
        
        top_indices = top_indices[valid_mask]
        top_scores = top_scores[valid_mask]
        
        return df.iloc[top_indices], top_scores.tolist()
        
    except Exception as e:
        st.warning(f"TF-IDF search failed: {e}")
        return simple_search(query, df, max_results)

def display_verse_card(verse_data, score=None):
    """Display a verse in a nice card format"""
    with st.container():
        st.markdown('<div class="verse-card">', unsafe_allow_html=True)
        
        # Header with surah info and score
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown(f"**📖 سورة {verse_data['surah']} - آية {verse_data['ayah_number']}**")
        
        with col2:
            if score is not None:
                percentage = int(score * 100)
                st.markdown(f"🎯 **{percentage}%**")
        
        # Arabic text
        st.markdown(f'<div class="arabic-text">{verse_data["text"]}</div>', 
                   unsafe_allow_html=True)
        
        # Tafseer if available
        if verse_data.get('tafseer') and verse_data['tafseer'].strip():
            st.markdown("**📚 التفسير:**")
            st.markdown(f"*{verse_data['tafseer']}*")
        
        st.markdown('</div>', unsafe_allow_html=True)

def show_package_status():
    """Show status of available packages"""
    st.markdown("### 📦 حالة المكتبات")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        status = "active" if PACKAGES_STATUS['datasets'] else "inactive"
        icon = "✅" if PACKAGES_STATUS['datasets'] else "❌"
        st.markdown(f'<span class="status-badge status-{status}">{icon} Datasets</span>', 
                   unsafe_allow_html=True)
    
    with col2:
        status = "active" if PACKAGES_STATUS['sklearn'] else "inactive"
        icon = "✅" if PACKAGES_STATUS['sklearn'] else "❌"
        st.markdown(f'<span class="status-badge status-{status}">{icon} Scikit-learn</span>', 
                   unsafe_allow_html=True)
    
    with col3:
        status = "active" if PACKAGES_STATUS['joblib'] else "inactive"
        icon = "✅" if PACKAGES_STATUS['joblib'] else "❌"
        st.markdown(f'<span class="status-badge status-{status}">{icon} Joblib</span>', 
                   unsafe_allow_html=True)

def main():
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>📖 تفسير القرآن الذكي</h1>
        <p>Smart Quran Tafseer - Cloud Version</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Load data
    df = load_quran_data()
    
    if df.empty:
        st.error("❌ لا يمكن تحميل البيانات")
        st.stop()
    
    # Show package status
    show_package_status()
    
    # Search interface
    st.markdown("### 🔍 البحث في القرآن الكريم")
    
    col1, col2 = st.columns([4, 1])
    
    with col1:
        search_query = st.text_input(
            "",
            placeholder="ابحث في القرآن... مثال: الرحمة، الصبر، الجنة",
            label_visibility="collapsed"
        )
    
    with col2:
        search_method = st.selectbox(
            "طريقة البحث",
            ["ذكي" if PACKAGES_STATUS['sklearn'] else "نصي بسيط", "نصي بسيط"]
        )
    
    # Advanced options
    with st.expander("⚙️ خيارات متقدمة"):
        max_results = st.slider("عدد النتائج", 5, 50, 10)
        search_in_tafseer = st.checkbox("البحث في التفسير أيضاً", value=True)
    
    # Perform search
    if search_query:
        with st.spinner("🔍 جاري البحث..."):
            # Prepare search dataframe
            search_df = df.copy()
            if search_in_tafseer:
                # Combine text and tafseer for search
                search_df['combined_text'] = (
                    search_df['clean_text'].fillna('') + ' ' + 
                    search_df['tafseer'].fillna('').apply(preprocess_arabic_text)
                )
                search_df['clean_text'] = search_df['combined_text']
            
            # Execute search
            if search_method == "ذكي" and PACKAGES_STATUS['sklearn']:
                results_df, scores = tfidf_search(search_query, search_df, max_results)
                search_type = "TF-IDF ذكي"
            else:
                results_df, scores = simple_search(search_query, search_df, max_results)
                search_type = "نصي بسيط"
            
            # Display results
            if not results_df.empty:
                st.success(f"🎯 تم العثور على {len(results_df)} نتيجة باستخدام البحث {search_type}")
                
                # Show average score
                if scores:
                    avg_score = np.mean(scores) * 100
                    st.info(f"📊 متوسط الدقة: {avg_score:.1f}%")
                
                # Display verses
                for idx, (_, verse) in enumerate(results_df.iterrows()):
                    score = scores[idx] if idx < len(scores) else None
                    display_verse_card(verse, score)
                    
            else:
                st.warning("❌ لم يتم العثور على نتائج. جرب كلمات مختلفة.")
    
    # Quick suggestions
    if not search_query:
        st.markdown("### 🚀 اقتراحات البحث")
        
        suggestions = ["الرحمة", "الصبر", "الجنة", "التوبة", "الدعاء", "العدل"]
        
        cols = st.columns(3)
        for i, suggestion in enumerate(suggestions):
            with cols[i % 3]:
                if st.button(f"🔍 {suggestion}", key=f"suggestion_{i}"):
                    st.session_state.search_query = suggestion
                    st.experimental_rerun()
    
    # Statistics
    st.markdown("### 📊 إحصائيات")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("إجمالي الآيات", len(df))
    
    with col2:
        unique_surahs = len(df['surah'].unique())
        st.metric("عدد السور", unique_surahs)
    
    with col3:
        avg_length = df['text'].str.len().mean()
        st.metric("متوسط طول الآية", f"{avg_length:.0f} حرف")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #666; padding: 1rem;">
        <p>📖 <strong>تفسير القرآن الذكي</strong> - نسخة مبسطة للسحابة</p>
        <p style="font-size: 0.9rem;">للاستفادة من جميع الميزات، قم بتشغيل التطبيق محلياً</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
