streamlit
pandas
numpy
scikit-learn
joblib
streamlit>=1.28.0
pandas>=1.5.0
numpy>=1.24.0
scikit-learn>=1.3.0
datasets>=2.14.0
transformers>=4.30.0
sentence-transformers>=2.2.0
gensim>=4.3.0
joblib>=1.3.0
arabic-reshaper>=3.0.0
python-bidi>=0.4.0
torch>=2.0.0
huggingface-hub>=0.16.0

## 🚀 Setup Instructions

### 1. Installation
```bash
# Clone or download the project
git clone <your-repo> 
cd quran-tafseer-app

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install requirements
pip install -r requirements.txt
```

### 2. Model Files Structure
```
quran-tafseer-app/
├── app.py                 # Main application file
├── requirements.txt       # Dependencies
├── bert_vectors.npy      # BERT embeddings (required)
├── w2v_vectors.npy       # Word2Vec document vectors (required)
├── word2vec.model        # Trained Word2Vec model (required)
├── README.md             # Documentation
└── models/               # Optional: model folder
    ├── preprocessing/    # Text preprocessing scripts
    └── training/         # Model training scripts
```

### 3. Generating Model Files

#### A. BERT Vectors (bert_vectors.npy)
```python
# Script to generate BERT vectors
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd

# Load dataset
dataset = load_dataset("MohamedRashad/Quran-Tafseer")
df = pd.DataFrame(dataset['train'])

# Load pre-trained Arabic BERT model
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
# Or use: 'aubmindlab/bert-base-arabertv02'

# Prepare texts
texts = df['text'].fillna('').tolist()  # or df['ayah'] depending on column name

# Generate embeddings
print("Generating BERT embeddings...")
embeddings = model.encode(texts, show_progress_bar=True, batch_size=32)

# Save vectors
np.save('bert_vectors.npy', embeddings)
print(f"BERT vectors saved: {embeddings.shape}")
```

#### B. Word2Vec Model and Vectors (word2vec.model & w2v_vectors.npy)
```python
# Script to generate Word2Vec model and document vectors
from datasets import load_dataset
from gensim.models import Word2Vec
import numpy as np
import pandas as pd
import re
from collections import defaultdict

def preprocess_arabic_text(text):
    """Enhanced Arabic text preprocessing"""
    if not isinstance(text, str):
        return ""
    
    # Remove diacritics
    tashkeel_pattern = re.compile(r'[\u064B-\u065F\u0670]')
    text = tashkeel_pattern.sub('', text)
    
    # Normalize Arabic letters
    text = re.sub(r'[إأآا]', 'ا', text)
    text = re.sub(r'ة', 'ه', text)
    text = re.sub(r'ي', 'ى', text)
    
    # Remove extra whitespace and punctuation
    text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# Load dataset
dataset = load_dataset("MohamedRashad/Quran-Tafseer")
df = pd.DataFrame(dataset['train'])

# Prepare corpus for Word2Vec training
print("Preprocessing texts...")
processed_texts = []
for text in df['text'].fillna(''):
    clean_text = preprocess_arabic_text(text)
    if clean_text:
        processed_texts.append(clean_text.split())

# Add tafseer texts if available
if 'tafseer' in df.columns:
    for text in df['tafseer'].fillna(''):
        clean_text = preprocess_arabic_text(text)
        if clean_text:
            processed_texts.append(clean_text.split())

print(f"Corpus size: {len(processed_texts)} documents")

# Train Word2Vec model
print("Training Word2Vec model...")
model = Word2Vec(
    sentences=processed_texts,
    vector_size=300,        # Embedding dimension
    window=10,              # Context window size
    min_count=2,            # Minimum word frequency
    workers=4,              # Number of threads
    epochs=100,             # Training epochs
    sg=1,                   # Skip-gram model
    hs=0,                   # Use negative sampling
    negative=10,            # Negative sampling rate
    seed=42                 # Random seed for reproducibility
)

# Save the trained model
model.save('word2vec.model')
print("Word2Vec model saved!")

# Generate document vectors using trained model
print("Generating document vectors...")
def get_document_vector(doc_words, model, vector_size=300):
    """Get document vector by averaging word vectors"""
    valid_words = [word for word in doc_words if word in model.wv]
    if not valid_words:
        return np.zeros(vector_size)
    
    word_vectors = [model.wv[word] for word in valid_words]
    return np.mean(word_vectors, axis=0)

# Create document vectors for the original dataset
doc_vectors = []
for text in df['text'].fillna(''):
    clean_text = preprocess_arabic_text(text)
    words = clean_text.split() if clean_text else []
    doc_vector = get_document_vector(words, model)
    doc_vectors.append(doc_vector)

# Convert to numpy array and save
doc_vectors = np.array(doc_vectors)
np.save('w2v_vectors.npy', doc_vectors)
print(f"Document vectors saved: {doc_vectors.shape}")

# Print model statistics
print(f"\nModel Statistics:")
print(f"Vocabulary size: {len(model.wv.key_to_index):,}")
print(f"Vector size: {model.wv.vector_size}")
print(f"Training epochs: {model.epochs}")

# Test similarity
try:
    similar_words = model.wv.most_similar('الله', topn=10)
    print(f"\nMost similar to 'الله': {similar_words}")
except:
    print("\n'الله' not found in vocabulary")
```

#### C. Alternative: Using Pre-trained Arabic Word2Vec
```python
# If you prefer using pre-trained Arabic Word2Vec models
import gensim.downloader as api

# Download pre-trained Arabic Word2Vec (if available)
# model = api.load('word2vec-arabic-wiki')  # Example name

# Or use AraVec models (you need to download separately)
# from gensim.models import KeyedVectors
# model = KeyedVectors.load_word2vec_format('path/to/aravec/model.txt')
```

### 4. Data Processing Scripts

#### A. Enhanced Text Preprocessing (preprocessing.py)
```python
import re
import string
from typing import List, Optional

class ArabicTextProcessor:
    """Enhanced Arabic text processing utilities"""
    
    def __init__(self):
        # Arabic diacritics pattern
        self.tashkeel_pattern = re.compile(r'[\u064B-\u065F\u0670]')
        
        # Arabic punctuation
        self.arabic_punctuation = '،؛؟!'
        
        # Stop words (basic list - you can expand this)
        self.stop_words = {
            'في', 'من', 'إلى', 'على', 'عن', 'مع', 'كل', 'بعض', 
            'هذا', 'هذه', 'ذلك', 'تلك', 'التي', 'الذي', 'التي',
            'وقد', 'فقد', 'كان', 'كانت', 'يكون', 'تكون', 'كما'
        }
    
    def remove_diacritics(self, text: str) -> str:
        """Remove Arabic diacritics"""
        return self.tashkeel_pattern.sub('', text)
    
    def normalize_arabic(self, text: str) -> str:
        """Normalize Arabic characters"""
        # Normalize Alef variations
        text = re.sub(r'[إأآا]', 'ا', text)
        # Normalize Teh Marbuta
        text = re.sub(r'ة', 'ه', text)
        # Normalize Yeh variations
        text = re.sub(r'[يئى]', 'ى', text)
        return text
    
    def clean_text(self, text: str, remove_stopwords: bool = False) -> str:
        """Complete text cleaning pipeline"""
        if not isinstance(text, str):
            return ""
        
        # Remove diacritics
        text = self.remove_diacritics(text)
        
        # Normalize characters
        text = self.normalize_arabic(text)
        
        # Remove non-Arabic characters except spaces
        text = re.sub(r'[^\u0600-\u06FF\s]', ' ', text)
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Remove stop words if requested
        if remove_stopwords:
            words = text.split()
            words = [word for word in words if word not in self.stop_words]
            text = ' '.join(words)
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """Tokenize Arabic text"""
        clean_text = self.clean_text(text)
        return clean_text.split()
    
    def extract_roots(self, text: str) -> List[str]:
        """Basic Arabic root extraction (simplified)"""
        # This is a simplified version - for production use Arabic NLP libraries
        words = self.tokenize(text)
        roots = []
        
        for word in words:
            if len(word) >= 3:
                # Very basic root extraction - remove common prefixes/suffixes
                root = word
                # Remove common prefixes
                for prefix in ['ال', 'و', 'ف', 'ب', 'ك', 'ل']:
                    if root.startswith(prefix):
                        root = root[len(prefix):]
                        break
                
                # Remove common suffixes
                for suffix in ['ها', 'ان', 'ين', 'ون', 'هم', 'كم']:
                    if root.endswith(suffix):
                        root = root[:-len(suffix)]
                        break
                
                if len(root) >= 3:
                    roots.append(root)
        
        return list(set(roots))  # Remove duplicates

# Usage example
processor = ArabicTextProcessor()
sample_text = "بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ"
clean_text = processor.clean_text(sample_text)
print(f"Original: {sample_text}")
print(f"Cleaned: {clean_text}")
```

### 5. Model Training Pipeline (train_models.py)
```python
#!/usr/bin/env python3
"""
Complete pipeline for training and saving all models
"""

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
from preprocessing import ArabicTextProcessor

def main():
    print("🚀 Starting Quran Tafseer Model Training Pipeline")
    
    # Initialize text processor
    processor = ArabicTextProcessor()
    
    # Load dataset
    print("📖 Loading Quran-Tafseer dataset...")
    dataset = load_dataset("MohamedRashad/Quran-Tafseer")
    df = pd.DataFrame(dataset['train'])
    print(f"✅ Loaded {len(df):,} records")
    
    # Prepare texts
    print("🔧 Preprocessing texts...")
    df['clean_text'] = df['text'].apply(processor.clean_text)
    df['clean_tafseer'] = df['tafseer'].fillna('').apply(processor.clean_text)
    
    texts = df['clean_text'].tolist()
    
    # 1. Train BERT embeddings
    print("\n🤖 Generating BERT embeddings...")
    try:
        # Try Arabic BERT model first
        bert_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        bert_embeddings = bert_model.encode(texts, show_progress_bar=True, batch_size=16)
        np.save('bert_vectors.npy', bert_embeddings)
        print(f"✅ BERT vectors saved: {bert_embeddings.shape}")
    except Exception as e:
        print(f"❌ BERT training failed: {e}")
    
    # 2. Train Word2Vec model
    print("\n🧠 Training Word2Vec model...")
    try:
        # Prepare corpus
        corpus = []
        for text in texts:
            if text.strip():
                corpus.append(text.split())
        
        # Add tafseer texts
        for text in df['clean_tafseer']:
            if text.strip():
                corpus.append(text.split())
        
        # Train model
        w2v_model = Word2Vec(
            sentences=corpus,
            vector_size=300,
            window=10,
            min_count=2,
            workers=4,
            epochs=100,
            sg=1,
            seed=42
        )
        
        # Save model
        w2v_model.save('word2vec.model')
        
        # Generate document vectors
        doc_vectors = []
        for text in texts:
            words = text.split()
            valid_words = [w for w in words if w in w2v_model.wv]
            if valid_words:
                word_vecs = [w2v_model.wv[w] for w in valid_words]
                doc_vec = np.mean(word_vecs, axis=0)
            else:
                doc_vec = np.zeros(300)
            doc_vectors.append(doc_vec)
        
        doc_vectors = np.array(doc_vectors)
        np.save('w2v_vectors.npy', doc_vectors)
        
        print(f"✅ Word2Vec model saved")
        print(f"✅ Document vectors saved: {doc_vectors.shape}")
        print(f"📊 Vocabulary size: {len(w2v_model.wv.key_to_index):,}")
        
    except Exception as e:
        print(f"❌ Word2Vec training failed: {e}")
    
    # 3. Train TF-IDF vectorizer (as backup)
    print("\n📊 Training TF-IDF vectorizer...")
    try:
        tfidf = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.95
        )
        tfidf.fit(texts)
        joblib.dump(tfidf, 'tfidf_vectorizer.pkl')
        print("✅ TF-IDF vectorizer saved")
    except Exception as e:
        print(f"❌ TF-IDF training failed: {e}")
    
    print("\n🎉 Model training pipeline completed!")
    print("\nGenerated files:")
    files = ['bert_vectors.npy', 'w2v_vectors.npy', 'word2vec.model', 'tfidf_vectorizer.pkl']
    for file in files:
        if os.path.exists(file):
            size = os.path.getsize(file) / (1024*1024)  # MB
            print(f"✅ {file} ({size:.1f} MB)")
        else:
            print(f"❌ {file} (not created)")

if __name__ == "__main__":
    main()
```

### 6. Running the Application

#### A. Quick Start
```bash
# After installing requirements and generating models
streamlit run app.py
```

#### B. With Custom Configuration
```bash
# Set environment variables for better performance
export STREAMLIT_SERVER_MAX_UPLOAD_SIZE=200
export STREAMLIT_SERVER_ENABLE_CORS=false

# Run with specific port
streamlit run app.py --server.port 8501
```

#### C. Production Deployment
```bash
# For production deployment
streamlit run app.py --server.address 0.0.0.0 --server.port 8501
```

### 7. Troubleshooting

#### Common Issues:

1. **Large Model Files**
   - BERT vectors can be 100+ MB
   - Word2Vec models can be 50+ MB
   - Use Git LFS for version control

2. **Memory Issues**
   - Reduce batch_size in BERT encoding
   - Use smaller vector_size in Word2Vec
   - Implement lazy loading for large models

3. **Arabic Text Issues**
   - Ensure UTF-8 encoding
   - Install Arabic fonts for proper display
   - Use proper RTL text direction

4. **Performance Optimization**
   - Cache models in session_state
   - Use @st.cache_data for expensive operations
   - Implement pagination for large result sets

### 8. Model Performance Tuning

#### BERT Optimization:
```python
# Use smaller models for faster inference
model_names = [
    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',  # Fast
    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',  # Balanced
    'aubmindlab/bert-base-arabertv02'  # Arabic-specific
]
```

#### Word2Vec Optimization:
```python
# Experiment with different parameters
Word2Vec(
    vector_size=200,     # Smaller for speed
    window=5,            # Smaller context
    min_count=3,         # Higher threshold
    epochs=50,           # Fewer epochs
    workers=8            # More workers if available
)
```

### 9. Data Sources and Licenses

- **Dataset**: MohamedRashad/Quran-Tafseer (Hugging Face)
- **Models**: Various open-source pre-trained models
- **License**: Ensure compliance with dataset and model licenses

### 10. Future Enhancements

- Add more Arabic NLP models (AraBERT, AraGPT)
- Implement semantic search with Elasticsearch
- Add voice search capabilities
- Multi-language support (translations)
- Advanced Arabic morphological analysis
- Integration with Islamic knowledge bases
